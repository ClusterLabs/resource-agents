#!/bin/sh
#
# 
# LVM
#
# Description:	Manages an LVM volume as an HA resource
#
#
# Author:	Alan Robertson
# Support:	linux-ha@lists.linux-ha.org
# License:	GNU General Public License (GPL)
# Copyright:	(C) 2002 - 2005 International Business Machines, Inc.
#
#	This code significantly inspired by the LVM resource
#	in FailSafe by Lars Marowsky-Bree
#
#
# An example usage in /etc/ha.d/haresources: 
#			 node1	10.0.0.170 ServeRAID::1::1 LVM::myvolname
#
# See usage() function below for more details...
#
#		OCF parameters are as below:
#		OCF_RESKEY_volgrpname
#		
#######################################################################
# Initialization:

: ${OCF_FUNCTIONS_DIR=${OCF_ROOT}/lib/heartbeat}
. ${OCF_FUNCTIONS_DIR}/ocf-shellfuncs

#######################################################################


usage() {
	methods=`LVM_methods`
	methods=`echo $methods | tr ' ' '|'`
	cat <<EOF
	usage: $0 $methods

	$0 manages an	Linux Volume Manager volume (LVM) as an HA resource

	The 'start' operation brings the given volume online
	The 'stop' operation takes the given volume offline
	The 'status' operation reports whether the volume is available
	The 'monitor' operation reports whether the volume seems present
	The 'validate-all' operation checks whether the OCF parameters are valid
	The 'meta-data' operation show meta data 
	The 'methods' operation reports on the methods $0 supports

EOF
}

# default for "tag"
OUR_TAG="pacemaker"

meta_data() {
	cat <<EOF
<?xml version="1.0"?>
<!DOCTYPE resource-agent SYSTEM "ra-api-1.dtd">
<resource-agent name="LVM">
<version>1.0</version>

<longdesc lang="en">
Resource script for LVM. It manages an Linux Volume Manager volume (LVM) 
as an HA resource. 
</longdesc>
<shortdesc lang="en">Controls the availability of an LVM Volume Group</shortdesc>

<parameters>
<parameter name="volgrpname" unique="0" required="1">
<longdesc lang="en">
The name of volume group.
</longdesc>
<shortdesc lang="en">Volume group name</shortdesc>
<content type="string" default="" />
</parameter>
<parameter name="exclusive" unique="0" required="0">
<longdesc lang="en">
If set, the volume group will be activated exclusively.  This option works one of
two ways.  If the volume group has the cluster attribute set, then the volume group
will be activated exclusively using clvmd across the cluster.  If the cluster attribute
is not set, the volume group will be activated exclusively through the use of the 
volume_list filter in lvm.conf. In the filter scenario, the LVM agent verifies
that pacemaker's configuration will result in the volume group only being active on a 
single node in the cluster and that the local node's volume_list filter will prevent 
the volume group from activating outside of the resource agent.  On activation this 
agent claims the volume group through the use of a unique tag, and then overrides the 
volume_list field in a way that allows the volume group to be activated only by the 
agent.  To use exclusive activation without clvmd, the volume_list in lvm.conf must be 
initialized. If volume groups exist locally that are not controlled by the cluster, such 
as the root volume group, make sure those volume groups are listed in the volume_list 
so they will be allowed to activate on bootup.
</longdesc>
<shortdesc lang="en">Exclusive activation</shortdesc>
<content type="boolean" default="false" />
</parameter>

<parameter name="tag" unique="0" required="0">
<longdesc lang="en">
If "exclusive" is set on a non clustered volume group, this overrides the tag to be used.
</longdesc>
<shortdesc lang="en">Exclusive activation tag</shortdesc>
<content type="string" default="$OUR_TAG" />
</parameter>

<parameter name="partial_activation" unique="0" required="0">
<longdesc lang="en">
If set, the volume group will be activated even only partial of the physical
volumes available. It helps to set to true, when you are using mirroring
logical volumes.
</longdesc>
<shortdesc lang="en">Activate VG even with partial PV only</shortdesc>
<content type="string" default="false" />
</parameter>

</parameters>

<actions>
<action name="start" timeout="30" />
<action name="stop" timeout="30" />
<action name="status" timeout="30" />
<action name="monitor" depth="0" timeout="30" interval="10" />
<action name="methods" timeout="5" />
<action name="meta-data" timeout="5" />
<action name="validate-all" timeout="5" />
</actions>
</resource-agent>
EOF
}

#
# methods: What methods/operations do we support?
#
LVM_methods() {
	cat <<EOF
	start
	stop
	status
	monitor
	methods
	validate-all
	meta-data
	usage
EOF
}

##
# returns mode
#
# 0 = normal (non-exclusive) local activation
# 1 = tagged-exclusive activation
# 2 = clvm-exclusive activation
##
VG_MODE=
get_vg_mode()
{
	if [ -n "$VG_MODE" ]; then
		echo "$VG_MODE"
		return
	fi

	VG_MODE=0
	if ocf_is_true "$OCF_RESKEY_exclusive"; then
		case $(vgs -o attr --noheadings $OCF_RESKEY_volgrpname | tr -d ' ') in
		?????c*)
			VG_MODE=2 ;;
		*)
			VG_MODE=1 ;;
		esac
	fi

	echo "$VG_MODE"
}

##
# Verify tags setup
##
verify_tags_environment()
{
	##
	# The volume_list must be initialized to something in order to
	# guarantee our tag will be filtered on startup
	##
	if ! lvm dumpconfig activation/volume_list; then
		ocf_log err "LVM: Improper setup detected"
		ocf_log err "The volume_list filter must be initialized in lvm.conf for exclusive activation without clvmd"
		return $OCF_ERR_GENERIC
	fi

	##
	# Our tag must _NOT_ be in the volume_list.  This agent
	# overrides the volume_list during activation using the
	# special tag reserved for cluster activation
	##
	if lvm dumpconfig activation/volume_list | grep -e "\"@$OUR_TAG\"" -e "\"${OCF_RESKEY_volgrpname}\""; then
		ocf_log err "LVM:  Improper setup detected"
		ocf_log err "The volume_list in lvm.conf must not contain the cluster tag, \"$OUR_TAG\", or volume group, $OCF_RESKEY_volgrpname"
		return $OCF_ERR_GENERIC
	fi


	##
	# Verify that if this agent is being used in a Pacemaker cluster,
	# no other LVM agents in the cib contain the same volgrpname
	##
	if have_binary "cibadmin" ; then
		# This detects if there is any other LVM primitive (besides this instance) that
		# is already assigned the same volume group. If so, do not activate this instance. 
		cibadmin --local -Q --xpath '//primitive[@type="LVM"][@id!="'${OCF_RESOURCE_INSTANCE}'"]//instance_attributes//nvpair[@name="volgrpname"][@value="'${OCF_RESKEY_volgrpname}'"]' \
			> /dev/null 2>&1

		if [ $? -eq 0 ]; then
			ocf_log err "LVM:  Improper setup detected"
			ocf_log err "Duplicate LVM resource controlling vg '$OCF_RESKEY_volgrpname' detected in cib configuration. Can not activate exclusively"
			return $OCF_ERR_GENERIC
		fi

	fi
	return $OCF_SUCCESS
}

check_initrd_warning()
{
	# First check to see if there is an initrd img we can safely
	# compare timestamps agaist.  If not, don't even bother with
	# this check.  This is known to work in rhel/fedora distros
	ls "/boot/*$(uname -r)*.img" > /dev/null 2>&1
	if [ $? -ne 0 ]; then
		return
	fi

	##
	# Now check to see if the initrd has been updated.
	# If not, the machine could boot and activate the VG outside
	# the control of pacemaker
	##
	if [ "$(find /boot -name *.img -newer /etc/lvm/lvm.conf)" = "" ]; then
		ocf_log warn "LVM:  Improper setup detected"
		ocf_log warn "* initrd image needs to be newer than lvm.conf"

		# While dangerous if not done the first time, there are many
		# cases where we don't simply want to fail here.  Instead,
		# keep warning until the user remakes the initrd - or has
		# it done for them by upgrading the kernel.
		#
		# initrd can be updated using this command.
		# dracut -H -f /boot/initramfs-$(uname -r).img $(uname -r)
		#
	fi
}

##
# does this vg have our tag
##
check_tags()
{
	local owner=`vgs -o tags --noheadings $OCF_RESKEY_volgrpname | tr -d ' '`

	if [ -z "$owner" ]; then
		# No-one owns this VG yet
		return 1
	fi

	if [ "$OUR_TAG" = "$owner" ]; then
		# yep, this is ours
		return 0
	fi

	# some other tag is set on this vg
	return 2
}

strip_tags()
{
	local i

	for i in `vgs --noheadings -o tags $OCF_RESKEY_volgrpname | sed s/","/" "/g`; do
		ocf_log info "Stripping tag, $i"

		# LVM version 2.02.98 allows changing tags if PARTIAL
		vgchange --deltag $i $OCF_RESKEY_volgrpname
	done

	if [ ! -z `vgs -o tags --noheadings $OCF_RESKEY_volgrpname | tr -d ' '` ]; then
		ocf_log err "Failed to remove ownership tags from $OCF_RESKEY_volgrpname"
		return $OCF_ERR_GENERIC
	fi

	return $OCF_SUCCESS
}

set_tags()
{
	check_tags
	case $? in
	0)
		# we already own it.
		return $OCF_SUCCESS
		;;
	2)
		# other tags are set, strip them before setting
		if ! strip_tags; then
			return $OCF_ERR_GENERIC
		fi
		;;
	*)
		: ;;
	esac

	vgchange --addtag $OUR_TAG $OCF_RESKEY_volgrpname
	if [ $? -ne 0 ]; then
		ocf_log err "Failed to add ownership tag to $OCF_RESKEY_volgrpname"
		return $OCF_ERR_GENERIC
	fi

	ocf_log info "New tag \"$OUR_TAG\" added to $OCF_RESKEY_volgrpname"
	return $OCF_SUCCESS
}

#
#	Return LVM status (silently)
#
LVM_status() {
	local rc=1
	loglevel="debug"

	# Set the log level of the error message
	if [ "X${2}" = "X" ]; then
		loglevel="err"
		if ocf_is_probe; then
			loglevel="warn"
		else 
			if [ ${OP_METHOD} = "stop" ]; then
				loglevel="info"
			fi
		fi
	fi
	
	if [ -d /dev/$1 ]; then
		test "`cd /dev/$1 && ls`" != ""
		rc=$?
		if [ $rc -ne 0 ]; then
			ocf_log err "VG $1 with no logical volumes is not supported by this RA!"
		fi
	fi

	if [ $rc -ne 0 ]; then
		ocf_log $loglevel "LVM Volume $1 is not available (stopped)"
		rc=$OCF_NOT_RUNNING
	else
		case $(get_vg_mode) in
		1) # exclusive with tagging.
			# If vg is running, make sure the correct tag is present. Otherwise we
			# can not guarantee exclusive activation.
			if ! check_tags; then
				ocf_log err "WARNING: $OCF_RESKEY_volgrpname is active without the cluster tag, \"$OUR_TAG\""
				rc=$OCF_ERR_GENERIC
			fi

			# make sure the environment for tags activation is still valid
			if ! verify_tags_environment; then
				rc=$OCF_ERR_GENERIC
			fi
			# let the user know if their initrd is older than lvm.conf.
			check_initrd_warning
			;;
		*)
			: ;;
		esac
	fi

	if [ "X${2}" = "X" ]; then
		# status call return
		return $rc
	fi

	# Report on LVM volume status to stdout...
	if [ $rc -eq 0 ]; then
		echo "Volume $1 is available (running)"
	else
		echo "Volume $1 is not available (stopped)"
	fi
	return $rc
}

get_activate_options()
{
	local options="-a"

	case $(get_vg_mode) in
	0) options="${options}ly";;
	1) options="${options}y --config activation{volume_list=[\"@${OUR_TAG}\"]}";;
	2) options="${options}ey";;
	esac

	if ocf_is_true "$OCF_RESKEY_partial_activation" ; then
		options="${options} --partial"
	fi

	# for clones (clustered volume groups), we'll also have to force
	# monitoring, even if disabled in lvm.conf.
	if ocf_is_clone; then
		options="$options --monitor y"
	fi
	
	echo $options
}

##
# Attempt to deactivate vg cluster wide and then start the vg exclusively
##
retry_exclusive_start()
{
	local vgchange_options=$(get_activate_options)

	# Deactivate each LV in the group one by one cluster wide
	set -- $(lvs -o name,attr --noheadings $OCF_RESKEY_volgrpname 2> /dev/null)
	while [ $# -ge 2 ]; do
		case $2 in
		????ao*)
			# open LVs cannot be deactivated.
			return $OCF_ERR_GENERIC;;
		*)
			if ! lvchange -an $OCF_RESKEY_volgrpname/$1; then
				ocf_log err "Unable to perform required deactivation of $OCF_RESKEY_volgrpname/$1 before starting"
				return $OCF_ERR_GENERIC
			fi
			;;
		esac
		shift 2
	done

	return ocf_run vgchange $vgchange_options $OCF_RESKEY_volgrpname
}

#
#	Enable LVM volume
#
LVM_start() {
	local vgchange_options=$(get_activate_options)
	local vg=$1
	local clvmd=0

	# TODO: This MUST run vgimport as well
	ocf_log info "Activating volume group $vg"
	if [ "$LVM_MAJOR" -eq "1" ]; then
		ocf_run vgscan $vg
	else
		ocf_run vgscan
	fi

	case $(get_vg_mode) in
	2)
		clvmd=1
		;;
	1)
		if ! set_tags; then
			return $OCF_ERR_GENERIC
		fi
		;;
	*)
		: ;;
	esac

	if ! ocf_run vgchange $vgchange_options $vg; then
		if [ $clvmd -eq 0 ]; then
			return $OCF_ERR_GENERIC
		fi

		# Failure to exclusively activate cluster vg.:
		# This could be caused by a remotely active LV, Attempt
		# to disable volume group cluster wide and try again.
		# Allow for some settling
		sleep 5
		if ! retry_exclusive_start; then
			return $OCF_ERR_GENERIC
		fi
	fi

	if LVM_status $vg; then
		: OK Volume $vg activated just fine!
		return $OCF_SUCCESS 
	else
		ocf_log err "LVM: $vg did not activate correctly"
		return $OCF_NOT_RUNNING
	fi
}

#
#	Disable the LVM volume
#
LVM_stop() {
	local res=$OCF_ERR_GENERIC
	local vgchange_options="-aln"
	local vg=$1

	if ! vgs $vg > /dev/null 2>&1; then
		ocf_log info "Volume group $vg not found"
		return $OCF_SUCCESS
	fi

	ocf_log info "Deactivating volume group $vg"

	case $(get_vg_mode) in
		1) vgchange_options="-an" ;;
	esac

	for i in $(seq 10)
	do
		ocf_run vgchange $vgchange_options $vg
		res=$?
		if LVM_status $vg; then
			ocf_log err "LVM: $vg did not stop correctly"
			res=1
		fi

		if [ $res -eq 0 ]; then
			break
		fi

		res=$OCF_ERR_GENERIC
		ocf_log warn "$vg still Active"
		ocf_log info "Retry deactivating volume group $vg"
		sleep 1
		which udevadm > /dev/null 2>&1 && udevadm settle --timeout=5
	done

	case $(get_vg_mode) in
	1)
		if [ $res -eq 0 ]; then
			strip_tags
			res=$?
		fi
		;;
	esac

	return $res
}

#
#	Check whether the OCF instance parameters are valid
#
LVM_validate_all() {
	check_binary $AWK

	##
	# Off-the-shelf tests...
	##
	VGOUT=`vgck ${VOLUME} 2>&1`
	if [ $? -ne 0 ]; then
		ocf_log err "Volume group [$VOLUME] does not exist or contains error! ${VGOUT}"
		exit $OCF_ERR_GENERIC
	fi

	##
	# Does the Volume Group exist?
	##
	if [ "$LVM_MAJOR" = "1" ]; then
		VGOUT=`vgdisplay ${VOLUME} 2>&1`
	else
		VGOUT=`vgdisplay -v ${VOLUME} 2>&1`
	fi
	if [ $? -ne 0 ]; then
		ocf_log err "Volume group [$VOLUME] does not exist or contains error! ${VGOUT}"
		exit $OCF_ERR_GENERIC
	fi

	##
	# If exclusive activation is not enabled, then
	# further checking of proper setup is not necessary
	##
	if ! ocf_is_true "$OCF_RESKEY_exclusive"; then
		return $OCF_SUCCESS;
	fi

	##
	# Having cloned lvm resources with exclusive vg activation makes no sense at all.
	##
	if ocf_is_clone; then
		ocf_log_err "cloned lvm resources can not be activated exclusively"
		exit $OCF_ERR_CONFIGURED
	fi

	##
	# Make sure the cluster attribute is set and clvmd is up when exclusive
	# activation is enabled. Otherwise we can't exclusively activate the volume group.
	##
	case $(get_vg_mode) in
	1)  # exclusive activation using tags
		if ! verify_tags_environment; then
			exit $OCF_ERR_GENERIC
		fi
		;;
	2)  # exclusive activation with clvmd
		##
		# verify is clvmd running
		##
		if ! ps -C clvmd > /dev/null 2>&1; then
			ocf_log err "$OCF_RESKEY_volgrpname has the cluster attribute set, but 'clvmd' is not running"
			exit $OCF_ERR_GENERIC
		fi
		;;
	*)
		: ;;
	esac

	return $OCF_SUCCESS
}

#
#	'main' starts here...
#

if
	[ $# -ne 1 ]
then
	usage
	exit $OCF_ERR_ARGS 
fi

case $1 in
	meta-data)	meta_data
		exit $OCF_SUCCESS;;

	methods)	LVM_methods
		exit $?;;

	usage)	usage
		exit $OCF_SUCCESS;;
	*)		;;
esac

if 
	[ -z "$OCF_RESKEY_volgrpname" ]
then
	ocf_log err "You must identify the volume group name!"
	exit $OCF_ERR_CONFIGURED 
fi

# Get the LVM version number, for this to work we assume(thanks to panjiam):
# 
# LVM1 outputs like this
#
#	# vgchange --version
#	vgchange: Logical Volume Manager 1.0.3
#	Heinz Mauelshagen, Sistina Software	19/02/2002 (IOP 10)
#
# LVM2 and higher versions output in this format
#
#	# vgchange --version
#	LVM version:		 2.00.15 (2004-04-19)
#	Library version: 1.00.09-ioctl (2004-03-31)
#	Driver version:	4.1.0

LVM_VERSION=`vgchange --version 2>&1 | \
	$AWK '/Logical Volume Manager/ {print $5"\n"; exit; }
			 /LVM version:/ {printf $3"\n"; exit;}'`
rc=$?

if
	( [ $rc -ne 0 ] || [ -z "$LVM_VERSION" ] )
then
	ocf_log err "LVM: $1 could not determine LVM version. Try 'vgchange --version' manually and modify $0 ?"
	exit $OCF_ERR_INSTALLED
fi
LVM_MAJOR="${LVM_VERSION%%.*}"

VOLUME=$OCF_RESKEY_volgrpname
OP_METHOD=$1

if [ -n "$OCF_RESKEY_tag" ]; then
	OUR_TAG=$OCF_RESKEY_tag
fi

# What kind of method was invoked?
case "$1" in

	start)
		LVM_validate_all
		LVM_start $VOLUME
		exit $?;;

	stop)	LVM_stop $VOLUME
		exit $?;;

	status)	LVM_status $VOLUME $1
		exit $?;;

	monitor)	LVM_status $VOLUME
		exit $?;;

	validate-all)	LVM_validate_all
		;;

	*)		usage
		exit $OCF_ERR_UNIMPLEMENTED;;
esac
