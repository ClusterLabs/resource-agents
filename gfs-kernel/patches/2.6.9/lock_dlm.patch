diff -urN linux-orig/fs/gfs_locking/lock_dlm/group.c linux-patched/fs/gfs_locking/lock_dlm/group.c
--- linux-orig/fs/gfs_locking/lock_dlm/group.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/group.c	2004-12-17 21:09:42.943443026 -0600
@@ -0,0 +1,827 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**  
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include "lock_dlm.h"
+
+
+struct kcl_service_ops mg_ops;
+
+/*
+ * Get the node struct for a given nodeid.
+ */
+
+static dlm_node_t *find_node_by_nodeid(dlm_t *dlm, uint32_t nodeid)
+{
+	dlm_node_t *node;
+
+	list_for_each_entry(node, &dlm->mg_nodes, list) {
+		if (node->nodeid == nodeid)
+			return node;
+	}
+	return NULL;
+}
+
+/*
+ * Get the node struct for a given journalid.
+ */
+
+static dlm_node_t *find_node_by_jid(dlm_t *dlm, uint32_t jid)
+{
+	dlm_node_t *node;
+
+	list_for_each_entry(node, &dlm->mg_nodes, list) {
+		if (node->jid == jid)
+			return node;
+	}
+	return NULL;
+}
+
+/*
+ * If the given ID is clear, get it, setting to the given VALUE.  The ID is a
+ * journalid, the VALUE is our nodeid.  When successful, the held ID-lock is
+ * returned (in shared mode).  As long as this ID-lock is held, the journalid
+ * is owned.
+ */
+
+static int id_test_and_set(dlm_t *dlm, uint32_t id, uint32_t val,
+			   dlm_lock_t **lp_set)
+{
+	dlm_lock_t *lp = NULL;
+	struct lm_lockname name;
+	lm_lock_t *lock;
+	char *lvb;
+	uint32_t exist_val, beval;
+	int error;
+
+	name.ln_type = LM_TYPE_JID;
+	name.ln_number = id;
+
+	error = lm_dlm_get_lock(dlm, &name, &lock);
+	if (error)
+		goto fail;
+
+	lp = (dlm_lock_t *) lock;
+
+	error = dlm_add_lvb(lp);
+	if (error)
+		goto fail_put;
+
+	lvb = lp->lvb;
+	set_bit(LFL_INLOCK, &lp->flags);
+	set_bit(LFL_NOBAST, &lp->flags);
+
+ retry:
+
+	error = lm_dlm_lock_sync(lock, LM_ST_UNLOCKED, LM_ST_SHARED,
+			         LM_FLAG_TRY | LM_FLAG_NOEXP);
+	if (error == -EAGAIN) {
+		current->state = TASK_UNINTERRUPTIBLE;
+		schedule_timeout(HZ);
+		goto retry;
+	}
+	if (error)
+		goto fail_lvb;
+
+	memcpy(&beval, lvb, sizeof(beval));
+	exist_val = be32_to_cpu(beval);
+
+	if (!exist_val) {
+		/*
+		 * This id is unused.  Attempt to claim it by getting EX mode
+		 * and writing our nodeid into the lvb.
+		 */
+		error = lm_dlm_lock_sync(lock, LM_ST_SHARED, LM_ST_EXCLUSIVE,
+				         LM_FLAG_TRY | LM_FLAG_NOEXP);
+		if (error == -EAGAIN) {
+			lm_dlm_unlock_sync(lock, LM_ST_SHARED);
+			current->state = TASK_UNINTERRUPTIBLE;
+			schedule_timeout(HZ);
+			goto retry;
+		}
+		if (error)
+			goto fail_unlock;
+
+		beval = cpu_to_be32(val);
+		memcpy(lvb, &beval, sizeof(beval));
+
+		error = lm_dlm_lock_sync(lock, LM_ST_EXCLUSIVE, LM_ST_SHARED,
+				         LM_FLAG_NOEXP);
+		DLM_ASSERT(!error,);
+
+		*lp_set = lp;
+		error = 0;
+	} else {
+		/*
+		 * This id is already used. It has a non-zero nodeid in the lvb
+		 */
+		lm_dlm_unlock_sync(lock, LM_ST_SHARED);
+		dlm_del_lvb(lp);
+		lm_dlm_put_lock(lock);
+		error = exist_val;
+	}
+
+	return error;
+
+ fail_unlock:
+	lm_dlm_unlock_sync(lock, LM_ST_SHARED);
+
+ fail_lvb:
+	dlm_del_lvb(lp);
+
+ fail_put:
+	lm_dlm_put_lock(lock);
+
+ fail:
+	return error;
+}
+
+/*
+ * Release a held ID-lock clearing its VALUE.  We have to acquire the lock in
+ * EX again so we can write out a zeroed lvb.
+ */
+
+static void id_clear(dlm_t *dlm, dlm_lock_t *lp)
+{
+	lm_lock_t *lock = (lm_lock_t *) lp;
+	int error;
+
+	/*
+	 * This flag means that DLM_LKF_CONVDEADLK should not be used.
+	 */
+	set_bit(LFL_FORCE_PROMOTE, &lp->flags);
+
+ retry:
+
+	error = lm_dlm_lock_sync(lock, LM_ST_SHARED, LM_ST_EXCLUSIVE,
+			         LM_FLAG_TRY | LM_FLAG_NOEXP);
+	if (error == -EAGAIN) {
+		schedule();
+		goto retry;
+	}
+	if (error)
+		goto end;
+
+	memset(lp->lvb, 0, DLM_LVB_LEN);
+	lm_dlm_unlock_sync(lock, LM_ST_EXCLUSIVE);
+
+ end:
+	dlm_del_lvb(lp);
+	lm_dlm_put_lock(lock);
+}
+
+/*
+ * Get the VALUE for a given ID.  The ID is a journalid, the VALUE is a nodeid.
+ */
+
+static int id_value(dlm_t *dlm, uint32_t id, uint32_t *val)
+{
+	dlm_lock_t *lp = NULL;
+	struct lm_lockname name;
+	lm_lock_t *lock;
+	char *lvb;
+	uint32_t beval;
+	int error;
+
+	name.ln_type = LM_TYPE_JID;
+	name.ln_number = id;
+
+	error = lm_dlm_get_lock(dlm, &name, &lock);
+	if (error)
+		goto out;
+
+	lp = (dlm_lock_t *) lock;
+
+	error = dlm_add_lvb(lp);
+	if (error)
+		goto out_put;
+
+	lvb = lp->lvb;
+	set_bit(LFL_INLOCK, &lp->flags);
+	set_bit(LFL_NOBAST, &lp->flags);
+
+ retry:
+
+	error = lm_dlm_lock_sync(lock, LM_ST_UNLOCKED, LM_ST_SHARED,
+			         LM_FLAG_TRY | LM_FLAG_NOEXP);
+	if (error == -EAGAIN) {
+		current->state = TASK_UNINTERRUPTIBLE;
+		schedule_timeout(HZ);
+		goto retry;
+	}
+	if (error)
+		goto out_lvb;
+
+	memcpy(&beval, lvb, sizeof(beval));
+	*val = be32_to_cpu(beval);
+
+	lm_dlm_unlock_sync(lock, LM_ST_SHARED);
+
+	error = 0;
+
+ out_lvb:
+	dlm_del_lvb(lp);
+
+ out_put:
+	lm_dlm_put_lock(lock);
+
+ out:
+	return error;
+}
+
+/*
+ * Find an ID with a given VALUE.  The ID is a journalid, the VALUE is a
+ * nodeid.
+ */
+
+static int id_find(dlm_t *dlm, uint32_t value, uint32_t *id_out)
+{
+	uint32_t val, id;
+	int error = 0, found = FALSE;
+
+	for (id = 0; id < dlm->max_nodes; id++) {
+		error = id_value(dlm, id, &val);
+		if (error)
+			break;
+
+		if (val == value) {
+			*id_out = id;
+			error = 0;
+			found = TRUE;
+			break;
+		}
+	}
+
+	if (!error && !found)
+		error = -ENOENT;
+
+	return error;
+}
+
+/*
+ * Get a journalid to use.  The journalid must be owned exclusively as long as
+ * this fs is mounted.  Other nodes must be able to discover our nodeid as the
+ * owner of the journalid.  The journalid we claim should have the lowest value
+ * of all unused journalids.
+ */
+
+static int claim_jid(dlm_t *dlm)
+{
+	dlm_node_t *node;
+	uint32_t id;
+	int error = 0;
+
+	DLM_ASSERT(dlm->our_nodeid,);
+
+	/*
+	 * Search an arbitrary number (8) past max nodes so we're sure to find
+	 * one so we can let the GFS handle the "too big jid" error and fail
+	 * the mount.
+	 */
+
+	for (id = 0; id < dlm->max_nodes + 8; id++) {
+		error = id_test_and_set(dlm, id, dlm->our_nodeid, &dlm->jid_lock);
+		if (error < 0)
+			break;
+		if (error > 0)
+			continue;
+
+		dlm->jid = id;
+		node = find_node_by_nodeid(dlm, dlm->our_nodeid);
+		node->jid = id;
+		set_bit(NFL_HAVE_JID, &node->flags);
+		break;
+	}
+
+	/*
+	 * If we have a problem getting a jid, pick a bogus one which should
+	 * cause GFS to complain and fail to mount.
+	 */
+
+	if (error) {
+		printk("lock_dlm: %s: no journal id available (%d)\n",
+		       dlm->fsname, error);
+		dlm->jid = dlm->max_nodes + dlm->our_nodeid;
+	}
+
+	log_debug("claim_jid %u", dlm->jid);
+	return 0;
+}
+
+/*
+ * Release our journalid, allowing it to be used by a node subsequently
+ * mounting the fs.
+ */
+
+static void release_jid(dlm_t *dlm)
+{
+	id_clear(dlm, dlm->jid_lock);
+	dlm->jid_lock = NULL;
+}
+
+/*
+ * For all nodes in the mountgroup, find the journalid being used by each.
+ */
+
+static int discover_jids(dlm_t *dlm)
+{
+	dlm_node_t *node;
+	uint32_t id;
+	int error, notfound = 0;
+
+	list_for_each_entry(node, &dlm->mg_nodes, list) {
+		if (test_bit(NFL_HAVE_JID, &node->flags))
+			continue;
+
+		error = id_find(dlm, node->nodeid, &id);
+		if (error) {
+			log_debug("jid for node %d not found", node->nodeid);
+			notfound++;
+			continue;
+		}
+
+		node->jid = id;
+		set_bit(NFL_HAVE_JID, &node->flags);
+	}
+
+	return notfound;
+}
+
+/*
+ * Discover the nodeid that we've been assigned by the cluster manager.
+ */
+
+static int get_our_nodeid(dlm_t *dlm)
+{
+	LIST_HEAD(cur_memb);
+	struct kcl_cluster_node *cur_node;
+
+	kcl_get_members(&cur_memb);
+
+	list_for_each_entry(cur_node, &cur_memb, list) {
+		if (cur_node->us) {
+			dlm->our_nodeid = cur_node->node_id;
+			break;
+		}
+	}
+
+	while (!list_empty(&cur_memb)) {
+		cur_node = list_entry(cur_memb.next, struct kcl_cluster_node,
+				      list);
+		list_del(&cur_node->list);
+		kfree(cur_node);
+	}
+
+	return 0;
+}
+
+/* 
+ * Run in dlm_async thread
+ */
+
+void process_start(dlm_t *dlm, dlm_start_t *ds)
+{
+	dlm_node_t *node;
+	uint32_t nodeid;
+	int last_stop, last_start, last_finish;
+	int error, i, new = FALSE, found;
+
+	down(&dlm->unmount_lock);
+
+	/*
+	 * when the initial sequence of callbacks is start/stop/start the
+	 * second start has the same event id (the first doesn't count)
+	 */
+
+	spin_lock(&dlm->async_lock);
+	last_stop = dlm->mg_last_stop;
+	last_start = dlm->mg_last_start;
+	last_finish = dlm->mg_last_finish;
+
+	if (!last_finish && last_stop) {
+		log_debug("pr_start reset stop %d start %d finish %d",
+			  last_stop, last_start, last_finish);
+		dlm->mg_last_stop = 0;
+		last_stop = 0;
+	}
+	spin_unlock(&dlm->async_lock);
+
+	log_debug("pr_start last_stop %d last_start %d last_finish %d",
+		  last_stop, last_start, last_finish);
+	log_debug("pr_start count %d type %d event %d flags %lx",
+		  ds->count, ds->type, ds->event_id, dlm->flags);
+
+	/*
+	 * gfs won't do journal recoveries once it's sent us an unmount
+	 */
+
+	if (test_bit(DFL_UMOUNT, &dlm->flags)) {
+		log_debug("pr_start %d skip for umount", ds->event_id);
+		kcl_start_done(dlm->mg_local_id, ds->event_id);
+		goto out;
+	}
+
+	/*
+	 * a couple special things to take care of on the first start (mount)
+	 */
+
+	if (!test_and_set_bit(DFL_GOT_NODEID, &dlm->flags))
+		get_our_nodeid(dlm);
+
+	if (test_bit(DFL_MOUNT, &dlm->flags) && (ds->count == 1))
+		set_bit(DFL_FIRST_MOUNT, &dlm->flags);
+
+	down(&dlm->mg_nodes_lock);
+
+	/*
+	 * while mounting, cancelled starts are discarded
+	 * (normally, (uninterrupted starts) mg_nodes is empty at this point)
+	 */
+
+	if (test_bit(DFL_MOUNT, &dlm->flags)) {
+		dlm_node_t *safe;
+		list_for_each_entry_safe(node, safe, &dlm->mg_nodes, list) {
+			list_del(&node->list);
+			kfree(node);
+		}
+	}
+
+	/*
+	 * find nodes which are gone
+	 */
+
+	list_for_each_entry(node, &dlm->mg_nodes, list) {
+		found = FALSE;
+		for (i = 0; i < ds->count; i++) {
+			if (node->nodeid != ds->nodeids[i])
+				continue;
+			found = TRUE;
+			break;
+		}
+		
+		/* node is still a member */
+		if (found)
+			continue;
+
+		set_bit(NFL_NOT_MEMBER, &node->flags);
+
+		/* no gfs recovery needed for nodes that left cleanly */
+		if (ds->type != SERVICE_NODE_FAILED)
+			continue;
+
+		/* callbacks sent only for nodes in last completed MG */
+		if (!test_bit(NFL_LAST_FINISH, &node->flags))
+			continue;
+
+		/* only send a single callback per node */
+		if (test_and_set_bit(NFL_SENT_CB, &node->flags))
+			continue;
+
+		dlm->fscb(dlm->fsdata, LM_CB_NEED_RECOVERY, &node->jid);
+		set_bit(DFL_NEED_STARTDONE, &dlm->flags);
+		log_debug("pr_start cb jid %u id %u", node->jid, node->nodeid);
+	}
+
+	/*
+	 * add new nodes
+	 */
+
+	for (i = 0; i < ds->count; i++) {
+		nodeid = ds->nodeids[i];
+
+		node = find_node_by_nodeid(dlm, nodeid);
+		if (node)
+			continue;
+
+		DLM_RETRY(node = kmalloc(sizeof(dlm_node_t), GFP_KERNEL), node);
+		memset(node, 0, sizeof(dlm_node_t));
+
+		node->nodeid = nodeid;
+		list_add(&node->list, &dlm->mg_nodes);
+		new = TRUE;
+	}
+
+	up(&dlm->mg_nodes_lock);
+
+	/*
+	 * get a jid for ourself when started for first time
+	 */
+
+	if (!test_and_set_bit(DFL_HAVE_JID, &dlm->flags))
+		claim_jid(dlm);
+	else if (new) {
+		/* give new nodes a little time to claim a jid */
+		current->state = TASK_INTERRUPTIBLE;
+		schedule_timeout(HZ);
+	}
+
+	/* 
+	 * find jid's of new nodes
+	 */
+
+	for (;;) {
+		/* we don't need to do these jid lookups if this start has been
+		   followed by a stop event (and thus cancelled) */
+
+		spin_lock(&dlm->async_lock);
+		last_stop = dlm->mg_last_stop;
+		last_start = dlm->mg_last_start;
+		spin_unlock(&dlm->async_lock);
+
+		if (last_stop >= ds->event_id) {
+			log_debug("pr_start %d abort discover", ds->event_id);
+			break;
+		}
+
+		error = discover_jids(dlm);
+		if (error) {
+			/* Not all jids were found.  Wait for a time to let all
+			   new nodes claim_jid, then try to scan for jids
+			   again. */
+			current->state = TASK_INTERRUPTIBLE;
+			schedule_timeout(HZ);
+			continue;
+		}
+		break;
+	}
+
+	/* 
+	 * tell SM we're done if there are no GFS recoveries to wait for
+	 */
+
+	if (last_start > last_stop) {
+		error = 0;
+		down(&dlm->mg_nodes_lock);
+
+		list_for_each_entry(node, &dlm->mg_nodes, list) {
+			if (!test_bit(NFL_SENT_CB, &node->flags))
+				continue;
+			error = 1;
+			break;
+		}
+		up(&dlm->mg_nodes_lock);
+
+		log_debug("pr_start %d done %d", ds->event_id, !error);
+
+		if (!error)
+			kcl_start_done(dlm->mg_local_id, ds->event_id);
+	} else
+		log_debug("pr_start %d stopped %d", ds->event_id, last_stop);
+
+ out:
+	clear_bit(DFL_RECOVER, &dlm->flags);
+	kfree(ds->nodeids);
+	kfree(ds);
+	up(&dlm->unmount_lock);
+}
+
+void process_finish(dlm_t *dlm)
+{
+	struct list_head *tmp, *tmpsafe;
+	dlm_node_t *node;
+	dlm_lock_t *lp;
+
+	log_debug("pr_finish flags %lx", dlm->flags);
+
+	spin_lock(&dlm->async_lock);
+	clear_bit(DFL_BLOCK_LOCKS, &dlm->flags);
+
+	list_for_each_safe(tmp, tmpsafe, &dlm->delayed) {
+		lp = list_entry(tmp, dlm_lock_t, dlist);
+
+		if (lp->type != QUEUE_LOCKS_BLOCKED)
+			continue;
+
+		lp->type = 0;
+		list_del(&lp->dlist);
+		list_add_tail(&lp->slist, &dlm->submit);
+
+		clear_bit(LFL_DLIST, &lp->flags);
+		set_bit(LFL_SLIST, &lp->flags);
+	}
+	spin_unlock(&dlm->async_lock);
+
+	down(&dlm->mg_nodes_lock);
+
+	list_for_each_safe(tmp, tmpsafe, &dlm->mg_nodes) {
+		node = list_entry(tmp, dlm_node_t, list);
+
+		if (test_bit(NFL_NOT_MEMBER, &node->flags)) {
+			list_del(&node->list);
+			kfree(node);
+		} else
+			set_bit(NFL_LAST_FINISH, &node->flags);
+	}
+	up(&dlm->mg_nodes_lock);
+
+	clear_bit(DFL_MOUNT, &dlm->flags);
+	wake_up(&dlm->wait);
+}
+
+/*
+ * Run in user process
+ */
+
+int init_mountgroup(dlm_t *dlm)
+{
+	int error;
+	int id;
+
+	error = kcl_register_service(dlm->fsname, dlm->fnlen, SERVICE_LEVEL_GFS,
+				     &mg_ops, TRUE, (void *) dlm, &id);
+	if (error)
+		goto out;
+
+	dlm->mg_local_id = id;
+
+	/* MOUNT and BLOCK_LOCKS are cleared when the join is finished */
+	set_bit(DFL_BLOCK_LOCKS, &dlm->flags);
+	set_bit(DFL_MOUNT, &dlm->flags);
+
+	error = kcl_join_service(id);
+	if (error)
+		goto out_unreg;
+
+	if (test_bit(DFL_START_ERROR, &dlm->flags))
+		goto out_leave;
+
+	return 0;
+
+ out_leave:
+	kcl_leave_service(dlm->mg_local_id);
+
+ out_unreg:
+	kcl_unregister_service(id);
+
+ out:
+	printk("lock_dlm: service error %d\n", error);
+	return error;
+}
+
+void release_mountgroup(dlm_t *dlm)
+{
+	int last_start, last_stop;
+
+	down(&dlm->unmount_lock);
+
+	/* this flag causes a kcl_start_done() to be sent right away for
+	   any start callbacks we get from SM */
+
+	log_debug("umount flags %lx", dlm->flags);
+	set_bit(DFL_UMOUNT, &dlm->flags);
+
+	/* gfs has done a unmount and will not call jid_recovery_done()
+	   any longer so make necessary kcl_start_done() calls so
+	   kcl_leave_service() will complete */
+
+	spin_lock(&dlm->async_lock);
+	last_start = dlm->mg_last_start;
+	last_stop = dlm->mg_last_stop;
+	spin_unlock(&dlm->async_lock);
+
+	if ((last_start > last_stop) &&
+	    test_and_clear_bit(DFL_NEED_STARTDONE, &dlm->flags)) {
+		log_debug("umount doing start_done %d", last_start);
+		kcl_start_done(dlm->mg_local_id, last_start);
+	}
+
+	up(&dlm->unmount_lock);
+
+	kcl_leave_service(dlm->mg_local_id);
+	kcl_unregister_service(dlm->mg_local_id);
+	release_jid(dlm);
+}
+
+/*
+ * Run in GFS thread
+ */
+
+void jid_recovery_done(dlm_t *dlm, unsigned int jid, unsigned int message)
+{
+	dlm_node_t *node;
+	int last_start, last_stop;
+	int remain = 0;
+
+	log_debug("recovery_done jid %u msg %u", jid, message);
+
+	node = find_node_by_jid(dlm, jid);
+	if (!node)
+		goto out;
+
+	log_debug("recovery_done nodeid %u flg %lx", node->nodeid, node->flags);
+
+	if (!test_bit(NFL_SENT_CB, &node->flags))
+		goto out;
+
+	if (!test_bit(NFL_NOT_MEMBER, &node->flags))
+		goto out;
+
+	set_bit(NFL_RECOVERY_DONE, &node->flags);
+
+	/* 
+	 * when recovery is done for all nodes, we're done with the start
+	 */
+
+	down(&dlm->mg_nodes_lock);
+
+	list_for_each_entry(node, &dlm->mg_nodes, list) {
+		if (test_bit(NFL_SENT_CB, &node->flags) &&
+		    !test_bit(NFL_RECOVERY_DONE, &node->flags))
+			remain++;
+	}
+	up(&dlm->mg_nodes_lock);
+
+	if (!remain) {
+		/* don't send a start_done if there's since been a stop which
+		 * cancels this start */
+
+		spin_lock(&dlm->async_lock);
+		last_start = dlm->mg_last_start;
+		last_stop = dlm->mg_last_stop;
+		spin_unlock(&dlm->async_lock);
+
+		if (last_start > last_stop) {
+			log_debug("recovery_done start_done %d", last_start);
+			kcl_start_done(dlm->mg_local_id, last_start);
+			clear_bit(DFL_NEED_STARTDONE, &dlm->flags);
+		}
+	}
+
+ out:
+	return;
+}
+
+/* 
+ * Run in CMAN SM thread
+ */
+
+static void queue_start(dlm_t *dlm, uint32_t *nodeids, int count,
+			int event_id, int type)
+{
+	dlm_start_t *ds;
+
+	DLM_RETRY(ds = kmalloc(sizeof(dlm_start_t), GFP_KERNEL), ds);
+
+	memset(ds, 0, sizeof(dlm_start_t));
+
+	ds->nodeids = nodeids;
+	ds->count = count;
+	ds->event_id = event_id;
+	ds->type = type;
+
+	spin_lock(&dlm->async_lock);
+	dlm->mg_last_start = event_id;
+	list_add_tail(&ds->list, &dlm->starts);
+	spin_unlock(&dlm->async_lock);
+
+	wake_up(&dlm->wait);
+}
+
+static int mg_stop(void *data)
+{
+	dlm_t *dlm = (dlm_t *) data;
+
+	spin_lock(&dlm->async_lock);
+	set_bit(DFL_BLOCK_LOCKS, &dlm->flags);
+	dlm->mg_last_stop = dlm->mg_last_start;
+	spin_unlock(&dlm->async_lock);
+
+	return 0;
+}
+
+static int mg_start(void *data, uint32_t *nodeids, int count, int event_id,
+		    int type)
+{
+	dlm_t *dlm = (dlm_t *) data;
+
+	queue_start(dlm, nodeids, count, event_id, type);
+
+	return 0;
+}
+
+static void mg_finish(void *data, int event_id)
+{
+	dlm_t *dlm = (dlm_t *) data;
+
+	spin_lock(&dlm->async_lock);
+	dlm->mg_last_finish = event_id;
+	set_bit(DFL_MG_FINISH, &dlm->flags);
+	spin_unlock(&dlm->async_lock);
+
+	wake_up(&dlm->wait);
+}
+
+struct kcl_service_ops mg_ops = {
+	.stop = mg_stop,
+	.start = mg_start,
+	.finish = mg_finish
+};
diff -urN linux-orig/fs/gfs_locking/lock_dlm/lock.c linux-patched/fs/gfs_locking/lock_dlm/lock.c
--- linux-orig/fs/gfs_locking/lock_dlm/lock.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/lock.c	2004-12-17 21:09:42.956440012 -0600
@@ -0,0 +1,704 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include "lock_dlm.h"
+
+static char junk_lvb[DLM_LVB_SIZE];
+
+
+/*
+ * Run in DLM thread
+ */
+
+static void queue_complete(dlm_lock_t *lp)
+{
+	dlm_t *dlm = lp->dlm;
+
+	if (!test_bit(LFL_WAIT_COMPLETE, &lp->flags)) {
+		printk("lock_dlm: extra completion %x,%"PRIx64" %d,%d id %x\n",
+		       lp->lockname.ln_type, lp->lockname.ln_number,
+		       lp->cur, lp->req, lp->lksb.sb_lkid);
+		return;
+	}
+
+	clear_bit(LFL_WAIT_COMPLETE, &lp->flags);
+
+	log_debug("qc %x,%"PRIx64" %d,%d id %x sts %d %x",
+		  lp->lockname.ln_type, lp->lockname.ln_number,
+		  lp->cur, lp->req, lp->lksb.sb_lkid, lp->lksb.sb_status,
+		  lp->lksb.sb_flags);
+
+	spin_lock(&dlm->async_lock);
+	list_add_tail(&lp->clist, &dlm->complete);
+	set_bit(LFL_CLIST, &lp->flags);
+	spin_unlock(&dlm->async_lock);
+	wake_up(&dlm->wait);
+}
+
+static void queue_blocking(dlm_lock_t *lp, int mode)
+{
+	dlm_t *dlm = lp->dlm;
+
+	if (test_bit(LFL_WAIT_COMPLETE, &lp->flags)) {
+		/* We often receive basts for EX while we're promoting
+		   from SH to EX. */
+		/* printk("lock_dlm: bast before complete %x,%"PRIx64" "
+		       "gr=%d rq=%d bast=%d\n", lp->lockname.ln_type,
+		       lp->lockname.ln_number, lp->cur, lp->req, mode); */
+		return;
+	}
+
+	if (!mode) {
+		printk("lock_dlm: bast mode zero %x,%"PRIx64"\n",
+			lp->lockname.ln_type, lp->lockname.ln_number);
+		return;
+	}
+
+	spin_lock(&dlm->async_lock);
+
+	if (!lp->bast_mode) {
+		list_add_tail(&lp->blist, &dlm->blocking);
+		set_bit(LFL_BLIST, &lp->flags);
+		lp->bast_mode = mode;
+	} else if (lp->bast_mode < mode)
+		lp->bast_mode = mode;
+
+	spin_unlock(&dlm->async_lock);
+	wake_up(&dlm->wait);
+}
+
+static __inline__ void lock_ast(void *astarg)
+{
+	queue_complete((dlm_lock_t *) astarg);
+}
+
+static __inline__ void lock_bast(void *astarg, int mode)
+{
+	queue_blocking((dlm_lock_t *) astarg, mode);
+}
+
+/*
+ * Run in GFS or user thread
+ */
+
+/**
+ * queue_delayed - add request to queue to be submitted later
+ * @lp: DLM lock
+ * @type: the reason the lock is blocked
+ *
+ * Queue of locks which need submitting sometime later.  Locks here
+ * due to BLOCKED_LOCKS are moved to request queue when recovery is
+ * done.  Locks here due to an ERROR are moved to request queue after
+ * some delay.  This could also be called from dlm_async thread.
+ */
+
+void queue_delayed(dlm_lock_t *lp, int type)
+{
+	dlm_t *dlm = lp->dlm;
+
+	lp->type = type;
+
+	spin_lock(&dlm->async_lock);
+	list_add_tail(&lp->dlist, &dlm->delayed);
+	set_bit(LFL_DLIST, &lp->flags);
+	spin_unlock(&dlm->async_lock);
+}
+
+/**
+ * make_mode - convert to DLM_LOCK_
+ * @lmstate: GFS lock state
+ *
+ * Returns: DLM lock mode
+ */
+
+static int16_t make_mode(int16_t lmstate)
+{
+	switch (lmstate) {
+	case LM_ST_UNLOCKED:
+		return DLM_LOCK_NL;
+	case LM_ST_EXCLUSIVE:
+		return DLM_LOCK_EX;
+	case LM_ST_DEFERRED:
+		return DLM_LOCK_CW;
+	case LM_ST_SHARED:
+		return DLM_LOCK_PR;
+	default:
+		DLM_ASSERT(0, printk("unknown LM state %d\n", lmstate););
+	}
+}
+
+/**
+ * make_lmstate - convert to LM_ST_
+ * @dlmmode: DLM lock mode 
+ *
+ * Returns: GFS lock state 
+ */
+
+int16_t make_lmstate(int16_t dlmmode)
+{
+	switch (dlmmode) {
+	case DLM_LOCK_IV:
+	case DLM_LOCK_NL:
+		return LM_ST_UNLOCKED;
+	case DLM_LOCK_EX:
+		return LM_ST_EXCLUSIVE;
+	case DLM_LOCK_CW:
+		return LM_ST_DEFERRED;
+	case DLM_LOCK_PR:
+		return LM_ST_SHARED;
+	default:
+		DLM_ASSERT(0, printk("unknown DLM mode %d\n", dlmmode););
+	}
+}
+
+/**
+ * check_cur_state - verify agreement with GFS on the current lock state
+ * @lp: the DLM lock 
+ * @cur_state: the current lock state from GFS
+ *
+ * NB: DLM_LOCK_NL and DLM_LOCK_IV are both considered 
+ * LM_ST_UNLOCKED by GFS.
+ *
+ */
+
+static void check_cur_state(dlm_lock_t *lp, unsigned int cur_state)
+{
+	int16_t cur = make_mode(cur_state);
+	if (lp->cur != DLM_LOCK_IV)
+		DLM_ASSERT(lp->cur == cur, printk("%d, %d\n", lp->cur, cur););
+}
+
+/**
+ * make_flags - put together necessary DLM flags
+ * @lp: DLM lock
+ * @gfs_flags: GFS flags
+ * @cur: current DLM lock mode
+ * @req: requested DLM lock mode
+ *
+ * Returns: DLM flags
+ */
+
+static unsigned int make_flags(dlm_lock_t *lp, unsigned int gfs_flags,
+			       int16_t cur, int16_t req)
+{
+	unsigned int lkf = 0;
+
+	if (gfs_flags & LM_FLAG_TRY)
+		lkf |= DLM_LKF_NOQUEUE;
+
+	if (gfs_flags & LM_FLAG_TRY_1CB) {
+		lkf |= DLM_LKF_NOQUEUE;
+		lkf |= DLM_LKF_NOQUEUEBAST;
+	}
+
+	if (gfs_flags & LM_FLAG_PRIORITY) {
+		lkf |= DLM_LKF_NOORDER;
+		lkf |= DLM_LKF_HEADQUE;
+	}
+
+	if (gfs_flags & LM_FLAG_ANY) {
+		if (req == DLM_LOCK_PR)
+			lkf |= DLM_LKF_ALTCW;
+		else if (req == DLM_LOCK_CW)
+			lkf |= DLM_LKF_ALTPR;
+	}
+
+	if (lp->lksb.sb_lkid != 0) {
+		lkf |= DLM_LKF_CONVERT;
+
+		/* Conversion deadlock avoidance by DLM */
+
+		if (!test_bit(LFL_FORCE_PROMOTE, &lp->flags) &&
+		    !(lkf & DLM_LKF_NOQUEUE) &&
+		    cur > DLM_LOCK_NL && req > DLM_LOCK_NL && cur != req)
+			lkf |= DLM_LKF_CONVDEADLK;
+	}
+
+	if (lp->lvb)
+		lkf |= DLM_LKF_VALBLK;
+
+	return lkf;
+}
+
+/**
+ * make_strname - convert GFS lock numbers to string
+ * @lockname: the lock type/number 
+ * @str: the lock string/length
+ *
+ */
+
+static __inline__ void make_strname(struct lm_lockname *lockname,
+				    strname_t *str)
+{
+	sprintf(str->name, "%8x%16"PRIx64, lockname->ln_type,
+		lockname->ln_number);
+	str->namelen = LOCK_DLM_STRNAME_BYTES;
+}
+
+int create_lp(dlm_t *dlm, struct lm_lockname *name, dlm_lock_t **lpp)
+{
+	dlm_lock_t *lp;
+
+	lp = kmalloc(sizeof(dlm_lock_t), GFP_KERNEL);
+	if (!lp)
+		return -ENOMEM;
+
+	memset(lp, 0, sizeof(dlm_lock_t));
+	lp->lockname = *name;
+	lp->dlm = dlm;
+	lp->cur = DLM_LOCK_IV;
+	lp->lvb = NULL;
+	lp->hold_null = NULL;
+	init_completion(&lp->uast_wait);
+	*lpp = lp;
+	return 0;
+}
+
+void delete_lp(dlm_lock_t *lp)
+{
+	spin_lock(&lp->dlm->async_lock);
+	if (test_bit(LFL_CLIST, &lp->flags)) {
+		printk("lock_dlm: dlm_put_lock lp on clist num=%x,%"PRIx64"\n",
+		       lp->lockname.ln_type, lp->lockname.ln_number);
+		list_del(&lp->clist);
+	}
+	if (test_bit(LFL_BLIST, &lp->flags)) {
+		/*
+		printk("lock_dlm: dlm_put_lock lp on blist num=%x,%"PRIx64"\n",
+		       lp->lockname.ln_type, lp->lockname.ln_number);
+		*/
+		list_del(&lp->blist);
+	}
+	if (test_bit(LFL_DLIST, &lp->flags)) {
+		printk("lock_dlm: dlm_put_lock lp on dlist num=%x,%"PRIx64"\n",
+		       lp->lockname.ln_type, lp->lockname.ln_number);
+		list_del(&lp->dlist);
+	}
+	if (test_bit(LFL_SLIST, &lp->flags)) {
+		printk("lock_dlm: dlm_put_lock lp on slist num=%x,%"PRIx64"\n",
+		       lp->lockname.ln_type, lp->lockname.ln_number);
+		list_del(&lp->slist);
+	}
+	spin_unlock(&lp->dlm->async_lock);
+
+	kfree(lp);
+}
+
+/**
+ * dlm_get_lock - get a lm_lock_t given a descripton of the lock
+ * @lockspace: the lockspace the lock lives in
+ * @name: the name of the lock
+ * @lockp: return the lm_lock_t here
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+int lm_dlm_get_lock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		    lm_lock_t **lockp)
+{
+	dlm_lock_t *lp;
+	int error;
+
+	error = create_lp((dlm_t *) lockspace, name, &lp);
+
+	*lockp = (lm_lock_t *) lp;
+	return error;
+}
+
+/**
+ * dlm_put_lock - get rid of a lock structure
+ * @lock: the lock to throw away
+ *
+ */
+
+void lm_dlm_put_lock(lm_lock_t *lock)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+	DLM_ASSERT(!lp->lvb,);
+	delete_lp(lp);
+}
+
+void do_dlm_unlock(dlm_lock_t *lp)
+{
+	unsigned int lkf = 0;
+	int error;
+
+	set_bit(LFL_DLM_UNLOCK, &lp->flags);
+	set_bit(LFL_WAIT_COMPLETE, &lp->flags);
+
+	if (lp->lvb)
+		lkf = DLM_LKF_VALBLK;
+
+	log_debug("un %x,%"PRIx64" %x %d %x", lp->lockname.ln_type,
+		  lp->lockname.ln_number, lp->lksb.sb_lkid, lp->cur, lkf);
+
+	error = dlm_unlock(lp->dlm->gdlm_lsp, lp->lksb.sb_lkid, lkf, NULL, lp);
+
+	DLM_ASSERT(!error, printk("%s: error=%d num=%x,%"PRIx64"\n",
+			      lp->dlm->fsname, error, lp->lockname.ln_type,
+			      lp->lockname.ln_number););
+}
+
+void do_dlm_unlock_sync(dlm_lock_t *lp)
+{
+	set_bit(LFL_UNLOCK_SYNC, &lp->flags);
+	init_completion(&lp->uast_wait);
+	do_dlm_unlock(lp);
+	wait_for_completion(&lp->uast_wait);
+}
+
+void do_dlm_lock(dlm_lock_t *lp, struct dlm_range *range)
+{
+	dlm_t *dlm = lp->dlm;
+	strname_t str;
+	int error, bast = 1;
+
+	/*
+	 * When recovery is in progress, delay lock requests for submission
+	 * once recovery is done.  Requests for recovery (NOEXP) and unlocks
+	 * can pass.
+	 */
+
+	if (test_bit(DFL_BLOCK_LOCKS, &dlm->flags) &&
+	    !test_bit(LFL_NOBLOCK, &lp->flags) && lp->req != DLM_LOCK_NL) {
+		queue_delayed(lp, QUEUE_LOCKS_BLOCKED);
+		return;
+	}
+
+	/*
+	 * Submit the actual lock request.
+	 */
+
+	if (lp->posix || test_bit(LFL_NOBAST, &lp->flags))
+		bast = 0;
+
+	make_strname(&lp->lockname, &str);
+
+	set_bit(LFL_WAIT_COMPLETE, &lp->flags);
+
+	log_debug("lk %x,%"PRIx64" id %x %d,%d %x", lp->lockname.ln_type,
+		  lp->lockname.ln_number, lp->lksb.sb_lkid,
+		  lp->cur, lp->req, lp->lkf);
+
+	error = dlm_lock(dlm->gdlm_lsp, lp->req, &lp->lksb, lp->lkf, str.name,
+			  str.namelen, 0, lock_ast, (void *) lp,
+			  bast ? lock_bast : NULL, range);
+
+	if ((error == -EAGAIN) && (lp->lkf & DLM_LKF_NOQUEUE)) {
+		lp->lksb.sb_status = -EAGAIN;
+		queue_complete(lp);
+		error = 0;
+	}
+
+	DLM_ASSERT(!error,
+		   printk("%s: num=%x,%"PRIx64" err=%d cur=%d req=%d lkf=%x\n",
+			  dlm->fsname, lp->lockname.ln_type,
+			  lp->lockname.ln_number, error, lp->cur, lp->req,
+			  lp->lkf););
+}
+
+int do_dlm_lock_sync(dlm_lock_t *lp, struct dlm_range *range)
+{
+	init_completion(&lp->uast_wait);
+	do_dlm_lock(lp, range);
+	wait_for_completion(&lp->uast_wait);
+
+	return lp->lksb.sb_status;
+}
+
+/**
+ * lm_dlm_lock - acquire a lock
+ * @lock: the lock to manipulate
+ * @cur_state: the current state
+ * @req_state: the requested state
+ * @flags: modifier flags
+ *
+ * Returns: A bitmap of LM_OUT_* on success, -EXXX on failure
+ */
+
+unsigned int lm_dlm_lock(lm_lock_t *lock, unsigned int cur_state,
+			 unsigned int req_state, unsigned int flags)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	if (flags & LM_FLAG_NOEXP)
+		set_bit(LFL_NOBLOCK, &lp->flags);
+
+	check_cur_state(lp, cur_state);
+	lp->req = make_mode(req_state);
+	lp->lkf = make_flags(lp, flags, lp->cur, lp->req);
+
+	do_dlm_lock(lp, NULL);
+	return LM_OUT_ASYNC;
+}
+
+int lm_dlm_lock_sync(lm_lock_t *lock, unsigned int cur_state,
+		     unsigned int req_state, unsigned int flags)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	init_completion(&lp->uast_wait);
+	lm_dlm_lock(lock, cur_state, req_state, flags);
+	wait_for_completion(&lp->uast_wait);
+
+	return lp->lksb.sb_status;
+}
+
+/**
+ * lm_dlm_unlock - unlock a lock
+ * @lock: the lock to manipulate
+ * @cur_state: the current state
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+unsigned int lm_dlm_unlock(lm_lock_t *lock, unsigned int cur_state)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	if (lp->cur == DLM_LOCK_IV)
+		return 0;
+	do_dlm_unlock(lp);
+	return LM_OUT_ASYNC;
+}
+
+void lm_dlm_unlock_sync(lm_lock_t *lock, unsigned int cur_state)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	init_completion(&lp->uast_wait);
+	lm_dlm_unlock(lock, cur_state);
+	wait_for_completion(&lp->uast_wait);
+}
+
+/**
+ * dlm_cancel - cancel a request that is blocked due to DFL_BLOCK_LOCKS
+ * @lock: the lock to cancel request for
+ *
+ */
+
+void lm_dlm_cancel(lm_lock_t *lock)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+	int dlist = FALSE;
+
+	printk("lock_dlm: cancel num=%x,%"PRIx64"\n",
+	       lp->lockname.ln_type, lp->lockname.ln_number);
+
+	spin_lock(&lp->dlm->async_lock);
+	if (test_and_clear_bit(LFL_DLIST, &lp->flags)) {
+		list_del(&lp->dlist);
+		lp->type = 0;
+		dlist = TRUE;
+	}
+	spin_unlock(&lp->dlm->async_lock);
+
+	if (dlist) {
+		set_bit(LFL_CANCEL, &lp->flags);
+		queue_complete(lp);
+	}
+}
+
+int dlm_add_lvb(dlm_lock_t *lp)
+{
+	char *lvb;
+
+	lvb = kmalloc(DLM_LVB_SIZE, GFP_KERNEL);
+	if (!lvb)
+		return -ENOMEM;
+
+	memset(lvb, 0, DLM_LVB_SIZE);
+
+	lp->lksb.sb_lvbptr = lvb;
+	lp->lvb = lvb;
+	return 0;
+}
+
+void dlm_del_lvb(dlm_lock_t *lp)
+{
+	kfree(lp->lvb);
+	lp->lvb = NULL;
+	lp->lksb.sb_lvbptr = NULL;
+}
+
+/**
+ * hold_null_lock - add a NL lock to the resource
+ * @lp: represents the resource
+ *
+ * This can do a synchronous dlm request (requiring a lock_dlm thread to
+ * get the completion) because gfs won't call hold_lvb() during a
+ * callback (from the context of a lock_dlm thread).
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+static int hold_null_lock(dlm_lock_t *lp)
+{
+	dlm_lock_t *lpn = NULL;
+	int error;
+
+	if (lp->hold_null) {
+		printk("lock_dlm: lvb already held\n");
+		return 0;
+	}
+
+	error = create_lp(lp->dlm, &lp->lockname, &lpn);
+	if (error)
+		goto out;
+
+	lpn->lksb.sb_lvbptr = junk_lvb;
+	lpn->lvb = junk_lvb;
+
+	lpn->req = DLM_LOCK_NL;
+	lpn->lkf = DLM_LKF_VALBLK;
+	set_bit(LFL_NOBAST, &lpn->flags);
+	set_bit(LFL_INLOCK, &lpn->flags);
+
+	error = do_dlm_lock_sync(lpn, NULL);
+	if (error) {
+		delete_lp(lpn);
+		lpn = NULL;
+	}
+
+ out:
+	lp->hold_null = lpn;
+	return error;
+}
+
+/**
+ * unhold_null_lock - remove the NL lock from the resource
+ * @lp: represents the resource
+ *
+ * This cannot do a synchronous dlm request (requiring a lock_dlm thread to
+ * get the completion) because gfs may call unhold_lvb() during a
+ * callback (from the context of a lock_dlm thread) which could cause a
+ * deadlock since the other lock_dlm thread could be engaged in recovery.
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+static void unhold_null_lock(dlm_lock_t *lp)
+{
+	dlm_lock_t *lpn = lp->hold_null;
+
+	lpn->lksb.sb_lvbptr = NULL;
+	lpn->lvb = NULL;
+	set_bit(LFL_UNLOCK_DELETE, &lpn->flags);
+	do_dlm_unlock(lpn);
+	lp->hold_null = NULL;
+}
+
+/**
+ * dlm_hold_lvb - hold on to a lock value block
+ * @lock: the lock the LVB is associated with
+ * @lvbp: return the lvb memory here
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+int lm_dlm_hold_lvb(lm_lock_t *lock, char **lvbp)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+	int error;
+
+	error = dlm_add_lvb(lp);
+	if (error)
+		return error;
+
+	*lvbp = lp->lvb;
+
+	/* Acquire a NL lock because gfs requires the value block to remain
+	   intact on the resource while the lvb is "held" even if it's holding
+	   no locks on the resource. */
+      
+	error = hold_null_lock(lp);
+	if (error)
+		dlm_del_lvb(lp);
+
+	return error;
+}
+
+/**
+ * dlm_unhold_lvb - release a LVB
+ * @lock: the lock the LVB is associated with
+ * @lvb: the lock value block
+ *
+ */
+
+void lm_dlm_unhold_lvb(lm_lock_t *lock, char *lvb)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	unhold_null_lock(lp);
+	dlm_del_lvb(lp);
+}
+
+/**
+ * dlm_sync_lvb - sync out the value of a lvb
+ * @lock: the lock the LVB is associated with
+ * @lvb: the lock value block
+ *
+ */
+
+void lm_dlm_sync_lvb(lm_lock_t *lock, char *lvb)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) lock;
+
+	if (lp->cur != DLM_LOCK_EX)
+		return;
+
+	init_completion(&lp->uast_wait);
+	set_bit(LFL_SYNC_LVB, &lp->flags);
+
+	lp->req = DLM_LOCK_EX;
+	lp->lkf = make_flags(lp, 0, lp->cur, lp->req);
+
+	do_dlm_lock(lp, NULL);
+	wait_for_completion(&lp->uast_wait);
+}
+
+/**
+ * dlm_recovery_done - reset the expired locks for a given jid
+ * @lockspace: the lockspace
+ * @jid: the jid
+ *
+ */
+
+void lm_dlm_recovery_done(lm_lockspace_t *lockspace, unsigned int jid,
+			  unsigned int message)
+{
+	jid_recovery_done((dlm_t *) lockspace, jid, message);
+}
+
+/*
+ * Run in dlm_async
+ */
+
+/**
+ * process_submit - make DLM lock requests from dlm_async thread
+ * @lp: DLM Lock
+ *
+ */
+
+void process_submit(dlm_lock_t *lp)
+{
+	struct dlm_range range, *r = NULL;
+
+	if (lp->posix) {
+		range.ra_start = lp->posix->start;
+		range.ra_end = lp->posix->end;
+		r = &range;
+	}
+
+	do_dlm_lock(lp, r);
+}
diff -urN linux-orig/fs/gfs_locking/lock_dlm/lock_dlm.h linux-patched/fs/gfs_locking/lock_dlm/lock_dlm.h
--- linux-orig/fs/gfs_locking/lock_dlm/lock_dlm.h	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/lock_dlm.h	2004-12-17 21:09:42.956440012 -0600
@@ -0,0 +1,375 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#ifndef LOCK_DLM_DOT_H
+#define LOCK_DLM_DOT_H
+
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/socket.h>
+#include <linux/kthread.h>
+#include <net/sock.h>
+#include <linux/lm_interface.h>
+#include <cluster/cnxman.h>
+#include <cluster/service.h>
+#include <cluster/dlm.h>
+
+/* We take a shortcut and use lm_lockname structs for internal locks.  This
+   means we must be careful to keep these types different from those used in
+   lm_interface.h. */
+
+#define LM_TYPE_JID		(0x10)
+#define LM_TYPE_PLOCK_UPDATE	(0x11)
+
+#define DLM_LVB_SIZE		(DLM_LVB_LEN)
+
+/* GFS uses 12 bytes to identify a resource (32 bit type + 64 bit number).
+   We sprintf these numbers into a 24 byte string of hex values to make them
+   human-readable (to make debugging simpler.) */
+
+#define LOCK_DLM_STRNAME_BYTES	(24)
+
+#define LOCK_DLM_MAX_NODES	(128)
+
+#define DROP_LOCKS_COUNT	(50000)
+#define DROP_LOCKS_PERIOD	(60)
+#define SHRINK_CACHE_COUNT	(100)
+#define SHRINK_CACHE_MAX	(1000)
+#define SHRINK_CACHE_TIME	(30)
+
+struct dlm;
+struct dlm_lock;
+struct dlm_node;
+struct dlm_start;
+struct strname;
+
+typedef struct dlm dlm_t;
+typedef struct dlm_lock dlm_lock_t;
+typedef struct dlm_node dlm_node_t;
+typedef struct dlm_start dlm_start_t;
+typedef struct strname strname_t;
+
+#define DFL_FIRST_MOUNT         0
+#define DFL_GOT_NODEID          1
+#define DFL_MG_FINISH           2
+#define DFL_HAVE_JID            3
+#define DFL_BLOCK_LOCKS         4
+#define DFL_START_ERROR         5
+#define DFL_MOUNT		6
+#define DFL_UMOUNT		7
+#define DFL_NEED_STARTDONE	8
+#define DFL_RECOVER		9
+
+struct dlm {
+	uint32_t		jid;
+	uint32_t		our_nodeid;
+	unsigned long		flags;
+
+	int			cnlen;
+	char *			clustername;
+	int			fnlen;
+	char *			fsname;
+	int			max_nodes;
+
+	dlm_lockspace_t *	gdlm_lsp;
+
+	lm_callback_t		fscb;
+	lm_fsdata_t *		fsdata;
+	dlm_lock_t *		jid_lock;
+
+	spinlock_t		async_lock;
+	struct list_head	complete;
+	struct list_head	blocking;
+	struct list_head	delayed;
+	struct list_head	submit;
+	struct list_head	starts;
+
+	wait_queue_head_t	wait;
+	struct task_struct *	thread1;
+	struct task_struct *	thread2;
+	atomic_t		lock_count;
+	unsigned long		drop_time;
+	unsigned long		shrink_time;
+
+	int			drop_locks_count;
+	int			drop_locks_period;
+
+	int			mg_local_id;
+	int			mg_last_start;
+	int			mg_last_stop;
+	int			mg_last_finish;
+	struct list_head	mg_nodes;
+	struct semaphore	mg_nodes_lock;
+	struct semaphore	unmount_lock;
+
+	struct list_head	resources;
+	struct semaphore	res_lock;
+	struct list_head	null_cache;
+	spinlock_t		null_cache_spin;
+	uint32_t		null_count;
+};
+
+struct dlm_resource {
+	dlm_t *                 dlm;
+	struct list_head        list;           /* list of resources */
+	struct lm_lockname      name;           /* the resource name */
+	struct semaphore        sema;
+	struct list_head        locks;          /* one lock for each range */
+	int                     count;
+	dlm_lock_t *		update;
+	struct list_head	async_locks;
+	spinlock_t		async_spin;
+	wait_queue_head_t	waiters;
+};
+
+struct posix_lock {
+	struct list_head        list;           /* resource locks list */
+	struct list_head	async_list;	/* resource async_locks list */
+	struct dlm_resource *   resource;
+	dlm_lock_t *            lp;
+	unsigned long           owner;
+	uint64_t                start;
+	uint64_t                end;
+	int                     count;
+	int                     ex;
+};
+
+#define LFL_NOBLOCK             0
+#define LFL_NOCACHE             1
+#define LFL_DLM_UNLOCK          2
+#define LFL_TRYFAILED           3
+#define LFL_SYNC_LVB            4
+#define LFL_FORCE_PROMOTE       5
+#define LFL_REREQUEST           6
+#define LFL_WAIT_COMPLETE       7
+#define LFL_CLIST               8
+#define LFL_BLIST               9
+#define LFL_DLIST               10
+#define LFL_SLIST               11
+#define LFL_INLOCK              12
+#define LFL_CANCEL              13
+#define LFL_UNLOCK_SYNC         14
+#define LFL_NOBAST              15
+#define LFL_HEADQUE             16
+#define LFL_UNLOCK_DELETE       17
+
+struct dlm_lock {
+	dlm_t *			dlm;
+	struct lm_lockname	lockname;
+	char *			lvb;
+	struct dlm_lksb		lksb;
+
+	int16_t			cur;
+	int16_t			req;
+	int16_t			prev_req;
+	unsigned int		lkf;
+	unsigned int		type;
+	unsigned long		flags;
+
+	int			bast_mode;	/* protected by async_lock */
+	struct completion	uast_wait;
+
+	struct list_head	clist;		/* complete */
+	struct list_head	blist;		/* blocking */
+	struct list_head	dlist;		/* delayed */
+	struct list_head	slist;		/* submit */
+
+	struct dlm_lock *	hold_null;	/* NL lock for hold_lvb */
+	struct posix_lock *	posix;
+	struct list_head	null_list;	/* NL lock cache for plocks */
+};
+
+#define NFL_SENT_CB             0
+#define NFL_NOT_MEMBER          1
+#define NFL_RECOVERY_DONE       2
+#define NFL_LAST_FINISH         3
+#define NFL_HAVE_JID            4
+
+struct dlm_node {
+	uint32_t		nodeid;
+	uint32_t		jid;
+	unsigned long		flags;
+	struct list_head	list;
+};
+
+#define QUEUE_LOCKS_BLOCKED     1
+#define QUEUE_ERROR_UNLOCK      2
+#define QUEUE_ERROR_LOCK        3
+#define QUEUE_ERROR_RETRY       4
+
+struct strname {
+	unsigned char		name[LOCK_DLM_STRNAME_BYTES];
+	unsigned short		namelen;
+};
+
+struct dlm_start {
+	uint32_t *		nodeids;
+	int			count;
+	int			type;
+	int			event_id;
+	struct list_head	list;
+};
+
+#ifndef TRUE
+#define TRUE (1)
+#endif
+
+#ifndef FALSE
+#define FALSE (0)
+#endif
+
+#if (BITS_PER_LONG == 64)
+#define PRIu64 "lu"
+#define PRId64 "ld"
+#define PRIo64 "lo"
+#define PRIx64 "lx"
+#define PRIX64 "lX"
+#define SCNu64 "lu"
+#define SCNd64 "ld"
+#define SCNo64 "lo"
+#define SCNx64 "lx"
+#define SCNX64 "lX"
+#else
+#define PRIu64 "Lu"
+#define PRId64 "Ld"
+#define PRIo64 "Lo"
+#define PRIx64 "Lx"
+#define PRIX64 "LX"
+#define SCNu64 "Lu"
+#define SCNd64 "Ld"
+#define SCNo64 "Lo"
+#define SCNx64 "Lx"
+#define SCNX64 "LX"
+#endif
+
+extern struct lm_lockops lock_dlm_ops;
+
+/* group.c */
+
+int init_mountgroup(dlm_t *dlm);
+void release_mountgroup(dlm_t *dlm);
+void process_start(dlm_t *dlm, dlm_start_t *ds);
+void process_finish(dlm_t *dlm);
+void jid_recovery_done(dlm_t *dlm, unsigned int jid, unsigned int message);
+
+/* thread.c */
+
+int init_async_thread(dlm_t *dlm);
+void release_async_thread(dlm_t *dlm);
+
+/* lock.c */
+
+int16_t make_lmstate(int16_t dlmmode);
+void queue_delayed(dlm_lock_t *lp, int type);
+void process_submit(dlm_lock_t *lp);
+int create_lp(dlm_t *dlm, struct lm_lockname *name, dlm_lock_t **lpp);
+void delete_lp(dlm_lock_t *lp);
+int dlm_add_lvb(dlm_lock_t *lp);
+void dlm_del_lvb(dlm_lock_t *lp);
+
+int lm_dlm_get_lock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		    lm_lock_t **lockp);
+void lm_dlm_put_lock(lm_lock_t *lock);
+
+void do_dlm_lock(dlm_lock_t *lp, struct dlm_range *range);
+int do_dlm_lock_sync(dlm_lock_t *lp, struct dlm_range *range);
+void do_dlm_unlock(dlm_lock_t *lp);
+void do_dlm_unlock_sync(dlm_lock_t *lp);
+
+unsigned int lm_dlm_lock(lm_lock_t *lock, unsigned int cur_state,
+			 unsigned int req_state, unsigned int flags);
+int lm_dlm_lock_sync(lm_lock_t *lock, unsigned int cur_state,
+		     unsigned int req_state, unsigned int flags);
+unsigned int lm_dlm_unlock(lm_lock_t *lock, unsigned int cur_state);
+void lm_dlm_unlock_sync(lm_lock_t *lock, unsigned int cur_state);
+
+void lm_dlm_cancel(lm_lock_t *lock);
+int lm_dlm_hold_lvb(lm_lock_t *lock, char **lvbp);
+void lm_dlm_unhold_lvb(lm_lock_t *lock, char *lvb);
+void lm_dlm_sync_lvb(lm_lock_t *lock, char *lvb);
+void lm_dlm_recovery_done(lm_lockspace_t *lockspace, unsigned int jid,
+		          unsigned int message);
+
+/* plock.c */
+
+int lm_dlm_plock_get(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		     struct file *file, struct file_lock *fl);
+int lm_dlm_plock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		 struct file *file, int cmd, struct file_lock *fl);
+int lm_dlm_punlock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		   struct file *file, struct file_lock *fl);
+void clear_null_cache(dlm_t *dlm);
+void shrink_null_cache(dlm_t *dlm);
+
+/* main.c */
+
+void lock_dlm_debug_log(const char *fmt, ...);
+void lock_dlm_debug_dump(void);
+
+
+#define LOCK_DLM_DEBUG
+
+#ifdef LOCK_DLM_DEBUG
+#define log_debug(fmt, args...) lock_dlm_debug_log(fmt, ##args)
+#else
+#define log_debug(fmt, args...)
+#endif
+
+#define log_all(fmt, args...) \
+	do { \
+		printk("lock_dlm: " fmt "\n", ##args); \
+		lock_dlm_debug_log(fmt, ##args); \
+	} while (0)
+
+#define log_error log_all
+
+
+static inline int check_timeout(unsigned long stamp, unsigned int seconds)
+{
+    return time_after(jiffies, stamp + seconds * HZ);
+}
+
+#define DLM_ASSERT(x, do) \
+{ \
+  if (!(x)) \
+  { \
+    dlm_locks_dump(); \
+    dlm_debug_dump(); \
+    lock_dlm_debug_dump(); \
+    printk("\nlock_dlm:  Assertion failed on line %d of file %s\n" \
+           "lock_dlm:  assertion:  \"%s\"\n" \
+	   "lock_dlm:  time = %lu\n", \
+	   __LINE__, __FILE__, #x, jiffies); \
+    {do} \
+    printk("\n"); \
+    BUG(); \
+    panic("lock_dlm:  Record message above and reboot.\n"); \
+  } \
+}
+
+#define DLM_RETRY(do_this, until_this) \
+for (;;) \
+{ \
+  do { do_this; } while (0); \
+  if (until_this) \
+    break; \
+  printk("lock_dlm:  out of memory:  %s, %u\n", __FILE__, __LINE__); \
+  schedule();\
+}
+
+#endif
diff -urN linux-orig/fs/gfs_locking/lock_dlm/main.c linux-patched/fs/gfs_locking/lock_dlm/main.c
--- linux-orig/fs/gfs_locking/lock_dlm/main.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/main.c	2004-12-17 21:09:42.956440012 -0600
@@ -0,0 +1,330 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include "lock_dlm.h"
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+
+#if defined(LOCK_DLM_DEBUG)
+#define LOCK_DLM_DEBUG_SIZE     (4096)
+#define MAX_DEBUG_MSG_LEN       (80)
+#else
+#define LOCK_DLM_DEBUG_SIZE     (0)
+#define MAX_DEBUG_MSG_LEN       (0)
+#endif
+#define MAX_PROC_STRING		(16)
+
+int				lock_dlm_max_nodes;
+int				lock_dlm_drop_count;
+int				lock_dlm_drop_period;
+
+static char *                   debug_buf;
+static unsigned int             debug_size;
+static unsigned int             debug_point;
+static int                      debug_wrap;
+static spinlock_t               debug_lock;
+static struct proc_dir_entry *	proc_dir = NULL;
+static char			proc_str[MAX_PROC_STRING + 1];
+
+
+void lock_dlm_debug_log(const char *fmt, ...)
+{
+	va_list va;
+	int i, n, size, len;
+	char buf[MAX_DEBUG_MSG_LEN+1];
+
+	spin_lock(&debug_lock);
+
+	if (!debug_buf)
+		goto out;
+
+	size = MAX_DEBUG_MSG_LEN;
+	memset(buf, 0, size+1);
+
+	n = 0;
+	/* n = snprintf(buf, size, "%s ", dlm->fsname); */
+	n = snprintf(buf, size, "%u ", current->pid);
+	size -= n;
+
+	va_start(va, fmt);
+	vsnprintf(buf+n, size, fmt, va);
+	va_end(va);
+
+	len = strlen(buf);
+	if (len > MAX_DEBUG_MSG_LEN-1)
+		len = MAX_DEBUG_MSG_LEN-1;
+	buf[len] = '\n';
+	buf[len+1] = '\0';
+
+	for (i = 0; i < strlen(buf); i++) {
+		debug_buf[debug_point++] = buf[i];
+
+		if (debug_point == debug_size) {
+			debug_point = 0;
+			debug_wrap = 1;
+		}
+	}
+ out:
+	spin_unlock(&debug_lock);
+}
+
+static void debug_setup(int size)
+{
+	char *b = NULL;
+
+	if (size > PAGE_SIZE)
+		size = PAGE_SIZE;
+	if (size)
+		b = kmalloc(size, GFP_KERNEL);
+
+	spin_lock(&debug_lock);
+	if (debug_buf)
+		kfree(debug_buf);
+	if (!size || !b)
+		goto out;
+	debug_size = size;
+	debug_point = 0;
+	debug_wrap = 0;
+	debug_buf = b;
+	memset(debug_buf, 0, debug_size);
+ out:
+	spin_unlock(&debug_lock);
+}
+
+static void debug_init(void)
+{
+	debug_buf = NULL;
+	debug_size = 0;
+	debug_point = 0;
+	debug_wrap = 0;
+	spin_lock_init(&debug_lock);
+	debug_setup(LOCK_DLM_DEBUG_SIZE);
+}
+
+void lock_dlm_debug_dump(void)
+{
+	int i;
+
+	spin_lock(&debug_lock);
+
+	if (debug_wrap) {
+		for (i = debug_point; i < debug_size; i++)
+			printk("%c", debug_buf[i]);
+	}
+	for (i = 0; i < debug_point; i++)
+		printk("%c", debug_buf[i]);
+
+	spin_unlock(&debug_lock);
+}
+
+EXPORT_SYMBOL(lock_dlm_debug_dump);
+
+#ifdef CONFIG_PROC_FS
+static int debug_info(char *b, char **start, off_t offset, int length)
+{
+	int i, n = 0;
+
+	spin_lock(&debug_lock);
+
+	if (debug_wrap) {
+		for (i = debug_point; i < debug_size; i++)
+			n += sprintf(b + n, "%c", debug_buf[i]);
+	}
+	for (i = 0; i < debug_point; i++)
+		n += sprintf(b + n, "%c", debug_buf[i]);
+
+	spin_unlock(&debug_lock);
+
+	return n;
+}
+
+static int max_nodes_info(char *b, char **start, off_t offset, int length)
+{
+	return sprintf(b, "%d\n", lock_dlm_max_nodes);
+}
+
+static int drop_count_info(char *b, char **start, off_t offset, int length)
+{
+	return sprintf(b, "%d\n", lock_dlm_drop_count);
+}
+
+static int drop_period_info(char *b, char **start, off_t offset, int length)
+{
+	return sprintf(b, "%d\n", lock_dlm_drop_period);
+}
+
+static int copy_string(const char *buffer, unsigned long count)
+{
+	int len;
+
+	if (count > MAX_PROC_STRING)
+		len = MAX_PROC_STRING;
+	else
+		len = count;
+
+	if (copy_from_user(proc_str, buffer, len))
+		return -EFAULT;
+	proc_str[len] = '\0';
+	return len;
+}
+
+static int max_nodes_write(struct file *file, const char *buffer,
+			   unsigned long count, void *data)
+{
+	int rv = copy_string(buffer, count);
+	if (rv < 0)
+		return rv;
+	lock_dlm_max_nodes = (int) simple_strtol(proc_str, NULL, 0);
+	return rv;
+}
+
+static int drop_count_write(struct file *file, const char *buffer,
+			    unsigned long count, void *data)
+{
+	int rv = copy_string(buffer, count);
+	if (rv < 0)
+		return rv;
+	lock_dlm_drop_count = (int) simple_strtol(proc_str, NULL, 0);
+	return rv;
+}
+
+static int drop_period_write(struct file *file, const char *buffer,
+			    unsigned long count, void *data)
+{
+	int rv = copy_string(buffer, count);
+	if (rv < 0)
+		return rv;
+	lock_dlm_drop_period = (int) simple_strtol(proc_str, NULL, 0);
+	return rv;
+}
+
+static void create_proc_entries(void)
+{
+	struct proc_dir_entry *p, *debug, *drop_count, *drop_period, *max_nodes;
+
+	debug = drop_count = drop_period = max_nodes = NULL;
+
+	proc_dir = proc_mkdir("cluster/lock_dlm", 0);
+	if (!proc_dir)
+		return;
+	proc_dir->owner = THIS_MODULE;
+
+	p = create_proc_entry("max_nodes", 0666, proc_dir);
+	if (!p)
+		goto out;
+	p->owner = THIS_MODULE;
+	p->get_info = max_nodes_info;
+	p->write_proc = max_nodes_write;
+	max_nodes = p;
+
+	p = create_proc_entry("debug", 0444, proc_dir);
+	if (!p)
+		goto out;
+	p->get_info = debug_info;
+	p->owner = THIS_MODULE;
+	debug = p;
+
+	p = create_proc_entry("drop_count", 0666, proc_dir);
+	if (!p)
+		goto out;
+	p->owner = THIS_MODULE;
+	p->get_info = drop_count_info;
+	p->write_proc = drop_count_write;
+	drop_count = p;
+
+	p = create_proc_entry("drop_period", 0666, proc_dir);
+	if (!p)
+		goto out;
+	p->owner = THIS_MODULE;
+	p->get_info = drop_period_info;
+	p->write_proc = drop_period_write;
+	drop_period = p;
+
+	return;
+
+ out:
+	if (drop_period)
+		remove_proc_entry("drop_period", proc_dir);
+	if (drop_count)
+		remove_proc_entry("drop_count", proc_dir);
+	if (debug)
+		remove_proc_entry("debug", proc_dir);
+	if (max_nodes)
+		remove_proc_entry("max_nodes", proc_dir);
+
+	remove_proc_entry("cluster/lock_dlm", NULL);
+	proc_dir = NULL;
+}
+
+static void remove_proc_entries(void)
+{
+	if (proc_dir) {
+		remove_proc_entry("max_nodes", proc_dir);
+		remove_proc_entry("debug", proc_dir);
+		remove_proc_entry("drop_period", proc_dir);
+		remove_proc_entry("drop_count", proc_dir);
+		remove_proc_entry("cluster/lock_dlm", NULL);
+		proc_dir = NULL;
+	}
+}
+#endif
+
+/**
+ * init_dlm - Initialize the dlm module
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+int __init init_lock_dlm(void)
+{
+	int error;
+
+	error = lm_register_proto(&lock_dlm_ops);
+	if (error) {
+		printk("lock_dlm:  can't register protocol: (%d)\n", error);
+		return error;
+	}
+
+	lock_dlm_max_nodes = LOCK_DLM_MAX_NODES;
+	lock_dlm_drop_count = DROP_LOCKS_COUNT;
+	lock_dlm_drop_period = DROP_LOCKS_PERIOD;
+
+#ifdef CONFIG_PROC_FS
+	create_proc_entries();
+#endif
+	debug_init();
+
+	printk("Lock_DLM (built %s %s) installed\n", __DATE__, __TIME__);
+	return 0;
+}
+
+/**
+ * exit_dlm - cleanup the dlm module
+ *
+ */
+
+void __exit exit_lock_dlm(void)
+{
+	lm_unregister_proto(&lock_dlm_ops);
+#ifdef CONFIG_PROC_FS
+	remove_proc_entries();
+#endif
+	debug_setup(0);
+}
+
+module_init(init_lock_dlm);
+module_exit(exit_lock_dlm);
+
+MODULE_DESCRIPTION("GFS DLM Locking Module");
+MODULE_AUTHOR("Red Hat, Inc.");
+MODULE_LICENSE("GPL");
diff -urN linux-orig/fs/gfs_locking/lock_dlm/mount.c linux-patched/fs/gfs_locking/lock_dlm/mount.c
--- linux-orig/fs/gfs_locking/lock_dlm/mount.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/mount.c	2004-12-17 21:09:42.957439780 -0600
@@ -0,0 +1,361 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include <linux/socket.h>
+#include <net/sock.h>
+
+#include "lock_dlm.h"
+#include <cluster/cnxman.h>
+#include <cluster/service.h>
+
+extern int lock_dlm_max_nodes;
+extern int lock_dlm_drop_count;
+extern int lock_dlm_drop_period;
+
+
+static int init_cman(dlm_t *dlm)
+{
+	int error = -1;
+	char *name = NULL;
+
+	if (!dlm->clustername)
+		goto fail;
+
+	error = kcl_addref_cluster();
+	if (error) {
+		printk("lock_dlm: cannot get cman reference %d\n", error);
+		goto fail;
+	}
+
+	error = kcl_cluster_name(&name);
+	if (error) {
+		printk("lock_dlm: cannot get cman cluster name %d\n", error);
+		goto fail_ref;
+	}
+
+	if (strcmp(name, dlm->clustername)) {
+		error = -1;
+		printk("lock_dlm: cman cluster name \"%s\" does not match "
+		       "file system cluster name \"%s\"\n",
+		       name, dlm->clustername);
+		goto fail_ref;
+	}
+
+	kfree(name);
+	return 0;
+
+ fail_ref:
+	kcl_releaseref_cluster();
+ fail:
+	if (name)
+		kfree(name);
+	return error;
+}
+
+static int release_cman(dlm_t *dlm)
+{
+	return kcl_releaseref_cluster();
+}
+
+static int init_cluster(dlm_t *dlm, char *table_name)
+{
+	char *buf, *c, *clname, *fsname;
+	int len, error = -1;
+
+	/*  
+	 * Parse superblock lock table <clustername>:<fsname>  
+	 */
+
+	len = strlen(table_name) + 1;
+	buf = kmalloc(len, GFP_KERNEL);
+	if (!buf)
+		goto out;
+	memset(buf, 0, len);
+	memcpy(buf, table_name, strlen(table_name));
+
+	c = strstr(buf, ":");
+	if (!c)
+		goto out_buf;
+
+	*c = '\0';
+	clname = buf;
+	fsname = ++c;
+
+	dlm->max_nodes = lock_dlm_max_nodes;
+
+	len = strlen(clname) + 1;
+	c = kmalloc(len, GFP_KERNEL);
+	if (!c)
+		goto out_buf;
+	memset(c, 0, len);
+	memcpy(c, clname, len-1);
+	dlm->cnlen = len-1;
+	dlm->clustername = c;
+
+	len = strlen(fsname) + 1;
+	c = kmalloc(len, GFP_KERNEL);
+	if (!c)
+		goto out_cn;
+	memset(c, 0, len);
+	memcpy(c, fsname, len-1);
+	dlm->fnlen = len-1;
+	dlm->fsname = c;
+
+	error = init_cman(dlm);
+	if (error)
+		goto out_fn;
+
+	kfree(buf);
+	return 0;
+
+ out_fn:
+	kfree(dlm->fsname);
+ out_cn:
+	kfree(dlm->clustername);
+ out_buf:
+	kfree(buf);
+ out:
+	printk("lock_dlm: init_cluster error %d\n", error);
+	return error;
+}
+
+static int release_cluster(dlm_t *dlm)
+{
+	release_cman(dlm);
+	kfree(dlm->clustername);
+	kfree(dlm->fsname);
+	return 0;
+}
+
+static int init_fence(dlm_t *dlm)
+{
+	LIST_HEAD(head);
+	struct kcl_service *s, *safe;
+	int error, found = FALSE;
+
+	error = kcl_get_services(&head, SERVICE_LEVEL_FENCE);
+	if (error < 0)
+		goto out;
+
+	list_for_each_entry_safe(s, safe, &head, list) {
+		list_del(&s->list);
+		if (!found && !strcmp(s->name, "default"))
+			found = TRUE;
+		kfree(s);
+	}
+
+	if (found)
+		return 0;
+
+	error = -1;
+ out:
+	printk("lock_dlm: fence domain not found; check fenced\n");
+	return error;
+}
+
+static int release_fence(dlm_t *dlm)
+{
+	return 0;
+}
+
+static int init_gdlm(dlm_t *dlm)
+{
+	int error;
+
+	error = dlm_new_lockspace(dlm->fsname, dlm->fnlen, &dlm->gdlm_lsp,
+				   DLM_LSF_NOTIMERS);
+	if (error)
+		printk("lock_dlm: new lockspace error %d\n", error);
+
+	return error;
+}
+
+static int release_gdlm(dlm_t *dlm)
+{
+	dlm_release_lockspace(dlm->gdlm_lsp, 2);
+	return 0;
+}
+
+static dlm_t *init_dlm(lm_callback_t cb, lm_fsdata_t *fsdata)
+{
+	dlm_t *dlm;
+
+	dlm = kmalloc(sizeof(dlm_t), GFP_KERNEL);
+	if (!dlm)
+		return NULL;
+
+	memset(dlm, 0, sizeof(dlm_t));
+
+	dlm->drop_locks_count = lock_dlm_drop_count;
+	dlm->drop_locks_period = lock_dlm_drop_period;
+
+	dlm->fscb = cb;
+	dlm->fsdata = fsdata;
+
+	spin_lock_init(&dlm->async_lock);
+
+	INIT_LIST_HEAD(&dlm->complete);
+	INIT_LIST_HEAD(&dlm->blocking);
+	INIT_LIST_HEAD(&dlm->delayed);
+	INIT_LIST_HEAD(&dlm->submit);
+	INIT_LIST_HEAD(&dlm->starts);
+	INIT_LIST_HEAD(&dlm->resources);
+	INIT_LIST_HEAD(&dlm->null_cache);
+
+	init_waitqueue_head(&dlm->wait);
+	dlm->thread1 = NULL;
+	dlm->thread2 = NULL;
+	atomic_set(&dlm->lock_count, 0);
+	dlm->drop_time = jiffies;
+	dlm->shrink_time = jiffies;
+
+	INIT_LIST_HEAD(&dlm->mg_nodes);
+	init_MUTEX(&dlm->mg_nodes_lock);
+	init_MUTEX(&dlm->unmount_lock);
+	init_MUTEX(&dlm->res_lock);
+
+	dlm->null_count = 0;
+	spin_lock_init(&dlm->null_cache_spin);
+
+	return dlm;
+}
+
+/**
+ * dlm_mount - mount a dlm lockspace
+ * @table_name: the name of the space to mount
+ * @host_data: host specific data
+ * @cb: the callback
+ * @lockstruct: the structure of crap to fill in
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+static int lm_dlm_mount(char *table_name, char *host_data,
+			lm_callback_t cb, lm_fsdata_t *fsdata,
+			unsigned int min_lvb_size,
+			struct lm_lockstruct *lockstruct)
+{
+	dlm_t *dlm;
+	int error = -ENOMEM;
+
+	if (min_lvb_size > DLM_LVB_SIZE)
+		goto out;
+
+	dlm = init_dlm(cb, fsdata);
+	if (!dlm)
+		goto out;
+
+	error = init_cluster(dlm, table_name);
+	if (error)
+		goto out_free;
+
+	error = init_fence(dlm);
+	if (error)
+		goto out_cluster;
+
+	error = init_gdlm(dlm);
+	if (error)
+		goto out_fence;
+
+	error = init_async_thread(dlm);
+	if (error)
+		goto out_gdlm;
+
+	error = init_mountgroup(dlm);
+	if (error)
+		goto out_thread;
+
+	lockstruct->ls_jid = dlm->jid;
+	lockstruct->ls_first = test_bit(DFL_FIRST_MOUNT, &dlm->flags);
+	lockstruct->ls_lockspace = dlm;
+	lockstruct->ls_ops = &lock_dlm_ops;
+	lockstruct->ls_flags = 0;
+	lockstruct->ls_lvb_size = DLM_LVB_SIZE;
+	return 0;
+
+ out_thread:
+	release_async_thread(dlm);
+ out_gdlm:
+	release_gdlm(dlm);
+ out_fence:
+	release_fence(dlm);
+ out_cluster:
+	release_cluster(dlm);
+ out_free:
+	kfree(dlm);
+ out:
+	return error;
+}
+
+/**
+ * dlm_others_may_mount
+ * @lockspace: the lockspace to unmount
+ *
+ */
+
+static void lm_dlm_others_may_mount(lm_lockspace_t *lockspace)
+{
+	/* Do nothing.  The first node to join the Mount Group will complete
+	 * before Service Manager allows another node to join. */
+}
+
+/**
+ * dlm_unmount - unmount a lock space
+ * @lockspace: the lockspace to unmount
+ *
+ */
+
+static void lm_dlm_unmount(lm_lockspace_t *lockspace)
+{
+	dlm_t *dlm = (dlm_t *) lockspace;
+
+	release_mountgroup(dlm);
+	release_async_thread(dlm);
+	release_gdlm(dlm);
+	release_fence(dlm);
+	release_cluster(dlm);
+	clear_null_cache(dlm);
+	kfree(dlm);
+}
+
+/**
+ * dlm_withdraw - withdraw from a lock space
+ * @lockspace: the lockspace to withdraw from
+ *
+ */
+
+static void lm_dlm_withdraw(lm_lockspace_t *lockspace)
+{
+	DLM_ASSERT(FALSE,);
+}
+
+struct lm_lockops lock_dlm_ops = {
+	lm_proto_name:"lock_dlm",
+	lm_mount:lm_dlm_mount,
+	lm_others_may_mount:lm_dlm_others_may_mount,
+	lm_unmount:lm_dlm_unmount,
+	lm_withdraw:lm_dlm_withdraw,
+	lm_get_lock:lm_dlm_get_lock,
+	lm_put_lock:lm_dlm_put_lock,
+	lm_lock:lm_dlm_lock,
+	lm_unlock:lm_dlm_unlock,
+	lm_plock:lm_dlm_plock,
+	lm_punlock:lm_dlm_punlock,
+	lm_plock_get:lm_dlm_plock_get,
+	lm_cancel:lm_dlm_cancel,
+	lm_hold_lvb:lm_dlm_hold_lvb,
+	lm_unhold_lvb:lm_dlm_unhold_lvb,
+	lm_sync_lvb:lm_dlm_sync_lvb,
+	lm_recovery_done:lm_dlm_recovery_done,
+	lm_owner:THIS_MODULE,
+};
diff -urN linux-orig/fs/gfs_locking/lock_dlm/plock.c linux-patched/fs/gfs_locking/lock_dlm/plock.c
--- linux-orig/fs/gfs_locking/lock_dlm/plock.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/plock.c	2004-12-17 21:09:42.957439780 -0600
@@ -0,0 +1,1236 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include "lock_dlm.h"
+#include <linux/fcntl.h>
+
+#define MIN(a,b) ((a) <= (b)) ? (a) : (b)
+#define MAX(a,b) ((a) >= (b)) ? (a) : (b)
+
+#define CREATE    1
+#define NO_CREATE 0
+
+#define WAIT      1
+#define NO_WAIT   0
+#define X_WAIT   -1
+
+#define EX        1
+#define NO_EX     0
+#define SH        NO_EX
+
+#define HEAD      1
+
+static int local_conflict(dlm_t *dlm, struct dlm_resource *r,
+			  struct lm_lockname *name, unsigned long owner,
+			  uint64_t start, uint64_t end, int ex);
+
+static int global_conflict(dlm_t *dlm, struct lm_lockname *name,
+			   unsigned long owner, uint64_t start, uint64_t end,
+			   int ex);
+
+/* remove lru lp from end of list, null_cache_spin must be held */
+
+static dlm_lock_t *lru_null(dlm_t *dlm)
+{
+	dlm_lock_t *lp;
+
+	lp = list_entry(dlm->null_cache.next, dlm_lock_t, null_list);
+	list_del(&lp->null_list);
+	dlm->null_count--;
+
+	return lp;
+}
+
+/* It's important that the lock_dlm thread not block doing any synchronous
+   dlm operations because a recovery event (which makes sync requests) can
+   happen during this.  If both lock_dlm threads do sync requests they deadlock
+   since one is required to process asts.  We also break out early if there's a
+   recovery so it doesn't have to wait. */
+
+void shrink_null_cache(dlm_t *dlm)
+{
+	dlm_lock_t *lp;
+
+	while (1) {
+		spin_lock(&dlm->null_cache_spin);
+		if (dlm->null_count <= SHRINK_CACHE_COUNT ||
+		    test_bit(DFL_RECOVER, &dlm->flags)) {
+			spin_unlock(&dlm->null_cache_spin);
+			break;
+		}
+
+		lp = lru_null(dlm);
+		spin_unlock(&dlm->null_cache_spin);
+
+		set_bit(LFL_UNLOCK_DELETE, &lp->flags);
+		do_dlm_unlock(lp);
+		schedule();
+	}
+}
+
+void clear_null_cache(dlm_t *dlm)
+{
+	dlm_lock_t *lp, *safe;
+
+	spin_lock(&dlm->null_cache_spin);
+	list_for_each_entry_safe(lp, safe, &dlm->null_cache, null_list) {
+		list_del(&lp->null_list);
+		dlm->null_count--;
+		delete_lp(lp);
+	}
+	spin_unlock(&dlm->null_cache_spin);
+}
+
+static void keep_null_lock(dlm_t *dlm, dlm_lock_t *lp)
+{
+	dlm_lock_t *lp2 = NULL;
+
+	spin_lock(&dlm->null_cache_spin);
+	/* add to front of list wrt list_add_tail/list_for_each */
+	list_add_tail(&lp->null_list, &dlm->null_cache);
+	dlm->null_count++;
+
+	/* help to shrink cache if too many null locks are piling up */
+	if (dlm->null_count > SHRINK_CACHE_MAX)
+		lp2 = lru_null(dlm);
+	spin_unlock(&dlm->null_cache_spin);
+
+	if (lp2) {
+		set_bit(LFL_UNLOCK_DELETE, &lp2->flags);
+		do_dlm_unlock(lp2);
+	}
+}
+
+static dlm_lock_t *find_null_lock(dlm_t *dlm, struct lm_lockname *name)
+{
+	dlm_lock_t *lp;
+	int found = FALSE;
+
+	spin_lock(&dlm->null_cache_spin);
+	list_for_each_entry(lp, &dlm->null_cache, null_list) {
+		if (lm_name_equal(&lp->lockname, name)) {
+			list_del(&lp->null_list);
+			dlm->null_count--;
+			found = TRUE;
+			break;
+		}
+	}
+	spin_unlock(&dlm->null_cache_spin);
+
+	if (!found)
+		lp = NULL;
+	return lp;
+}
+
+static int lock_resource(struct dlm_resource *r)
+{
+	dlm_lock_t *lp;
+	struct lm_lockname name;
+	int error;
+
+	name.ln_type = LM_TYPE_PLOCK_UPDATE;
+	name.ln_number = r->name.ln_number;
+
+	lp = find_null_lock(r->dlm, &name);
+	if (!lp) {
+		error = create_lp(r->dlm, &name, &lp);
+		if (error)
+			return error;
+		set_bit(LFL_NOBAST, &lp->flags);
+		set_bit(LFL_INLOCK, &lp->flags);
+	} else
+		lp->lkf = DLM_LKF_CONVERT;
+
+	lp->req = DLM_LOCK_EX;
+	error = do_dlm_lock_sync(lp, NULL);
+	if (error) {
+		delete_lp(lp);
+		lp = NULL;
+	}
+
+	r->update = lp;
+	return error;
+}
+
+static void unlock_resource(struct dlm_resource *r)
+{
+	dlm_lock_t *lp = r->update;
+
+	set_bit(LFL_NOBAST, &lp->flags);
+	set_bit(LFL_INLOCK, &lp->flags);
+	lp->req = DLM_LOCK_NL;
+	lp->lkf = DLM_LKF_CONVERT;
+	do_dlm_lock_sync(lp, NULL);
+	keep_null_lock(r->dlm, lp);
+	r->update = NULL;
+}
+
+static struct dlm_resource *search_resource(dlm_t *dlm, struct lm_lockname *name)
+{
+	struct dlm_resource *r;
+
+	list_for_each_entry(r, &dlm->resources, list) {
+		if (lm_name_equal(&r->name, name))
+			return r;
+	}
+	return NULL;
+}
+
+static int get_resource(dlm_t *dlm, struct lm_lockname *name, int create,
+			struct dlm_resource **res)
+{
+	struct dlm_resource *r, *r2;
+	int error = -ENOMEM;
+
+	down(&dlm->res_lock);
+	r = search_resource(dlm, name);
+	if (r)
+		r->count++;
+	up(&dlm->res_lock);
+
+	if (r)
+		goto out;
+
+	if (create == NO_CREATE) {
+		error = -ENOENT;
+		goto fail;
+	}
+
+	r = kmalloc(sizeof(struct dlm_resource), GFP_KERNEL);
+	if (!r)
+		goto fail;
+
+	memset(r, 0, sizeof(struct dlm_resource));
+	r->dlm = dlm;
+	r->name = *name;
+	r->count = 1;
+	INIT_LIST_HEAD(&r->locks);
+	INIT_LIST_HEAD(&r->async_locks);
+	init_MUTEX(&r->sema);
+	spin_lock_init(&r->async_spin);
+	init_waitqueue_head(&r->waiters);
+
+	down(&dlm->res_lock);
+	r2 = search_resource(dlm, name);
+	if (r2) {
+		r2->count++;
+		up(&dlm->res_lock);
+		kfree(r);
+		r = r2;
+		goto out;
+	}
+
+	list_add_tail(&r->list, &dlm->resources);
+	up(&dlm->res_lock);
+
+ out:
+	*res = r;
+	return 0;
+ fail:
+	return error;
+}
+
+static void put_resource(struct dlm_resource *r)
+{
+	dlm_t *dlm = r->dlm;
+
+	down(&dlm->res_lock);
+	r->count--;
+	if (r->count == 0) {
+		DLM_ASSERT(list_empty(&r->locks), );
+		DLM_ASSERT(list_empty(&r->async_locks), );
+		list_del(&r->list);
+		kfree(r);
+	}
+	up(&dlm->res_lock);
+}
+
+static inline void hold_resource(struct dlm_resource *r)
+{
+	down(&r->dlm->res_lock);
+	r->count++;
+	up(&r->dlm->res_lock);
+}
+
+static inline int ranges_overlap(uint64_t start1, uint64_t end1,
+				 uint64_t start2, uint64_t end2)
+{
+	if (end1 < start2 || start1 > end2)
+		return FALSE;
+	return TRUE;
+}
+
+/**
+ * overlap_type - returns a value based on the type of overlap
+ * @s1 - start of new lock range
+ * @e1 - end of new lock range
+ * @s2 - start of existing lock range
+ * @e2 - end of existing lock range
+ *
+ */
+
+static int overlap_type(uint64_t s1, uint64_t e1, uint64_t s2, uint64_t e2)
+{
+	int ret;
+
+	/*
+	 * ---r1---
+	 * ---r2---
+	 */
+
+	if (s1 == s2 && e1 == e2)
+		ret = 0;
+
+	/*
+	 * --r1--
+	 * ---r2---
+	 */
+
+	else if (s1 == s2 && e1 < e2)
+		ret = 1;
+
+	/*
+	 *   --r1--
+	 * ---r2---
+	 */
+
+	else if (s1 > s2 && e1 == e2)
+		ret = 1;
+
+	/*
+	 *  --r1--
+	 * ---r2---
+	 */
+
+	else if (s1 > s2 && e1 < e2)
+		ret = 2;
+
+	/*
+	 * ---r1---  or  ---r1---  or  ---r1---
+	 * --r2--          --r2--       --r2--
+	 */
+
+	else if (s1 <= s2 && e1 >= e2)
+		ret = 3;
+
+	/*
+	 *   ---r1---
+	 * ---r2---
+	 */
+
+	else if (s1 > s2 && e1 > e2)
+		ret = 4;
+
+	/*
+	 * ---r1---
+	 *   ---r2---
+	 */
+
+	else if (s1 < s2 && e1 < e2)
+		ret = 4;
+
+	else
+		ret = -1;
+
+	return ret;
+}
+
+/* shrink the range start2:end2 by the partially overlapping start:end */
+
+static int shrink_range2(uint64_t *start2, uint64_t *end2,
+			 uint64_t start, uint64_t end)
+{
+	int error = 0;
+
+	if (*start2 < start)
+		*end2 = start - 1;
+	else if (*end2 > end)
+		*start2 =  end + 1;
+	else
+		error = -1;
+	return error;
+}
+
+static int shrink_range(struct posix_lock *po, uint64_t start, uint64_t end)
+{
+	return shrink_range2(&po->start, &po->end, start, end);
+}
+
+static void put_lock(dlm_lock_t *lp)
+{
+	struct posix_lock *po = lp->posix;
+
+	po->count--;
+	if (po->count == 0) {
+		kfree(po);
+		delete_lp(lp);
+	}
+}
+
+static int create_lock(struct dlm_resource *r, unsigned long owner, int ex,
+		       uint64_t start, uint64_t end, dlm_lock_t **lpp)
+{
+	dlm_lock_t *lp;
+	struct posix_lock *po;
+	int error;
+
+	error = create_lp(r->dlm, &r->name, &lp);
+	if (error)
+		return error;
+
+	po = kmalloc(sizeof(struct posix_lock), GFP_KERNEL);
+	if (!po) {
+		kfree(lp);
+		return -ENOMEM;
+	}
+	memset(po, 0, sizeof(struct posix_lock));
+
+	lp->posix = po;
+	po->lp = lp;
+	po->resource = r;
+	po->count = 1;
+	po->start = start;
+	po->end = end;
+	po->owner = owner;
+	po->ex = ex;
+	list_add_tail(&po->list, &r->locks);
+
+	*lpp = lp;
+	return 0;
+}
+
+static unsigned int make_flags_posix(dlm_lock_t *lp, int wait)
+{
+	unsigned int lkf = DLM_LKF_NOORDER;
+
+	if (test_and_clear_bit(LFL_HEADQUE, &lp->flags))
+		lkf |= DLM_LKF_HEADQUE;
+
+	if (wait == NO_WAIT || wait == X_WAIT)
+		lkf |= DLM_LKF_NOQUEUE;
+
+	if (lp->lksb.sb_lkid != 0)
+		lkf |= DLM_LKF_CONVERT;
+
+	return lkf;
+}
+
+static void do_range_lock(dlm_lock_t *lp)
+{
+	struct dlm_range range = { lp->posix->start, lp->posix->end };
+	do_dlm_lock(lp, &range);
+}
+
+static void request_lock(dlm_lock_t *lp, int wait)
+{
+	set_bit(LFL_INLOCK, &lp->flags);
+	lp->req = lp->posix->ex ? DLM_LOCK_EX : DLM_LOCK_PR;
+	lp->lkf = make_flags_posix(lp, wait);
+
+	log_debug("req %x,%"PRIx64" %s %"PRIx64"-%"PRIx64" lkf %x wait %u",
+		  lp->lockname.ln_type, lp->lockname.ln_number,
+		  lp->posix->ex ? "ex" : "sh", lp->posix->start,
+		  lp->posix->end, lp->lkf, wait);
+
+	do_range_lock(lp);
+}
+
+static void add_async(struct posix_lock *po, struct dlm_resource *r)
+{
+	spin_lock(&r->async_spin);
+	list_add_tail(&po->async_list, &r->async_locks);
+	spin_unlock(&r->async_spin);
+}
+
+static void del_async(struct posix_lock *po, struct dlm_resource *r)
+{
+	spin_lock(&r->async_spin);
+	list_del(&po->async_list);
+	spin_unlock(&r->async_spin);
+}
+
+static int wait_async(dlm_lock_t *lp)
+{
+	wait_for_completion(&lp->uast_wait);
+	del_async(lp->posix, lp->posix->resource);
+	return lp->lksb.sb_status;
+}
+
+static void wait_async_list(struct dlm_resource *r, unsigned long owner)
+{
+	struct posix_lock *po;
+	int error, found;
+
+ restart:
+	found = FALSE;
+	spin_lock(&r->async_spin);
+	list_for_each_entry(po, &r->async_locks, async_list) {
+		if (po->owner != owner)
+			continue;
+		found = TRUE;
+		break;
+	}
+	spin_unlock(&r->async_spin);
+
+	if (found) {
+		DLM_ASSERT(po->lp, );
+		error = wait_async(po->lp);
+		DLM_ASSERT(!error, );
+		goto restart;
+	}
+}
+
+static void update_lock(dlm_lock_t *lp, int wait)
+{
+	request_lock(lp, wait);
+	add_async(lp->posix, lp->posix->resource);
+
+	if (wait == NO_WAIT || wait == X_WAIT) {
+		int error = wait_async(lp);
+		DLM_ASSERT(!error, printk("error=%d\n", error););
+	}
+}
+
+static void add_lock(struct dlm_resource *r, unsigned long owner, int wait,
+		     int ex, uint64_t start, uint64_t end, int head)
+{
+	dlm_lock_t *lp;
+	int error;
+
+	error = create_lock(r, owner, ex, start, end, &lp);
+	DLM_ASSERT(!error, );
+	if (head == HEAD)
+		set_bit(LFL_HEADQUE, &lp->flags);
+
+	hold_resource(r);
+	update_lock(lp, wait);
+}
+
+static int remove_lock(dlm_lock_t *lp)
+{
+	struct dlm_resource *r = lp->posix->resource;
+
+	log_debug("remove %x,%"PRIx64"", r->name.ln_type, r->name.ln_number);
+
+	do_dlm_unlock_sync(lp);
+	put_lock(lp);
+	put_resource(r);
+	return 0;
+}
+
+/* RN within RE (and starts or ends on RE boundary)
+   1. add new lock for non-overlap area of RE, orig mode
+   2. convert RE to RN range and mode */
+
+static int lock_case1(struct posix_lock *po, struct dlm_resource *r,
+		      unsigned long owner, int wait, int ex, uint64_t start,
+		      uint64_t end)
+{
+	uint64_t start2, end2;
+
+	/* non-overlapping area start2:end2 */
+	start2 = po->start;
+	end2 = po->end;
+	shrink_range2(&start2, &end2, start, end);
+
+	po->start = start;
+	po->end = end;
+	po->ex = ex;
+
+	if (ex) {
+		add_lock(r, owner, X_WAIT, SH, start2, end2, HEAD);
+		update_lock(po->lp, wait);
+	} else {
+		add_lock(r, owner, WAIT, EX, start2, end2, HEAD);
+		update_lock(po->lp, X_WAIT);
+	}
+	return 0;
+}
+
+/* RN within RE (RE overlaps RN on both sides)
+   1. add new lock for front fragment, orig mode
+   2. add new lock for back fragment, orig mode
+   3. convert RE to RN range and mode */
+			 
+static int lock_case2(struct posix_lock *po, struct dlm_resource *r,
+		      unsigned long owner, int wait, int ex, uint64_t start,
+		      uint64_t end)
+{
+	if (ex) {
+		add_lock(r, owner, X_WAIT, SH, po->start, start-1, HEAD);
+		add_lock(r, owner, X_WAIT, SH, end+1, po->end, HEAD);
+
+		po->start = start;
+		po->end = end;
+		po->ex = ex;
+
+		update_lock(po->lp, wait);
+	} else {
+		add_lock(r, owner, WAIT, EX, po->start, start-1, HEAD);
+		add_lock(r, owner, WAIT, EX, end+1, po->end, HEAD);
+
+		po->start = start;
+		po->end = end;
+		po->ex = ex;
+
+		update_lock(po->lp, X_WAIT);
+	}
+	return 0;
+}
+
+/* returns ranges from exist list in order of their start values */
+
+static int next_exist(struct list_head *exist, uint64_t *start, uint64_t *end)
+{
+	struct posix_lock *po;
+	int first = TRUE, first_call = FALSE;
+
+	if (!*start && !*end)
+		first_call = TRUE;
+
+	list_for_each_entry(po, exist, list) {
+		if (!first_call && (po->start <= *start))
+			continue;
+
+		if (first) {
+			*start = po->start;
+			*end = po->end;
+			first = FALSE;
+		} else if (po->start < *start) {
+			*start = po->start;
+			*end = po->end;
+		}
+	}
+
+	return (first ? -1 : 0);
+}
+
+/* adds locks in gaps between existing locks from start to end */
+
+static int fill_gaps(struct list_head *exist, struct dlm_resource *r,
+		     unsigned long owner, int wait, int ex, uint64_t start,
+		     uint64_t end)
+{
+	uint64_t exist_start = 0, exist_end = 0;
+
+	/* cover gaps in front of each existing lock */
+	for (;;) {
+		if (next_exist(exist, &exist_start, &exist_end))
+			break;
+		if (start < exist_start)
+			add_lock(r, owner, wait, ex, start, exist_start-1, 0);
+		start = exist_end + 1;
+	}
+
+	/* cover gap after last existing lock */
+	if (exist_end < end)
+		add_lock(r, owner, wait, ex, exist_end+1, end, 0);
+
+	return 0;
+}
+
+/* RE within RN (possibly more than one RE lock, all within RN) */
+
+static int lock_case3(struct list_head *exist, struct dlm_resource *r,
+		      unsigned long owner, int wait, int ex, uint64_t start,
+		      uint64_t end)
+{
+	struct posix_lock *po, *safe;
+
+	fill_gaps(exist, r, owner, wait, ex, start, end);
+
+	if (!ex)
+		wait = X_WAIT;
+
+	/* update existing locks to new mode and put back in locks list */
+	list_for_each_entry_safe(po, safe, exist, list) {
+		list_move_tail(&po->list, &r->locks);
+		if (po->ex == ex)
+			continue;
+		po->ex = ex;
+		update_lock(po->lp, wait);
+	}
+
+	return 0;
+}
+
+/* RE within RN (possibly more than one RE lock, one RE partially overlaps RN)
+   1. add new locks with new mode for RN gaps not covered by RE's
+   2. convert RE locks' mode to new mode
+   other steps deal with the partial-overlap fragment and depend on whether
+   the request is sh->ex or ex->sh */
+
+static int lock_case4(struct posix_lock *opo, struct list_head *exist,
+		      struct dlm_resource *r, unsigned long owner, int wait,
+		      int ex, uint64_t start, uint64_t end)
+{
+	struct posix_lock *po, *safe;
+	uint64_t over_start = 0, over_end = 0;
+	uint64_t frag_start = 0, frag_end = 0;
+
+	/* fragment (non-overlap) range of opo */
+	if (opo->start < start) {
+		frag_start = opo->start;
+		frag_end = start - 1;
+	} else {
+		frag_start = end + 1;
+		frag_end = opo->end;
+	}
+
+	/* overlap range of opo */
+	if (opo->start < start) {
+		over_start = start;
+		over_end = opo->end;
+	} else {
+		over_start = opo->start;
+		opo->end = end;
+	}
+
+	/* cut off the non-overlap portion of opo so fill_gaps will work */
+	opo->start = over_start;
+	opo->end = over_end;
+
+	fill_gaps(exist, r, owner, wait, ex, start, end);
+
+	/* update existing locks to new mode and put back in locks list */
+	list_for_each_entry_safe(po, safe, exist, list) {
+		list_move_tail(&po->list, &r->locks);
+		if (po == opo)
+			continue;
+		if (po->ex == ex)
+			continue;
+		po->ex = ex;
+		update_lock(po->lp, wait);
+	}
+
+	/* deal with the RE that partially overlaps the requested range */
+
+	if (ex == opo->ex)
+		return 0;
+
+	if (ex) {
+		/* 1. add a shared lock in the non-overlap range
+		   2. convert RE to overlap range and requested mode */
+
+		add_lock(r, owner, X_WAIT, SH, frag_start, frag_end, HEAD);
+
+		opo->start = over_start;
+		opo->end = over_end;
+		opo->ex = ex;
+
+		update_lock(opo->lp, wait);
+	} else {
+		/* 1. request a shared lock in the overlap range
+		   2. convert RE to non-overlap range
+		   3. wait for shared lock to complete */
+
+		add_lock(r, owner, WAIT, SH, over_start, over_end, HEAD);
+
+		opo->start = frag_start;
+		opo->end = frag_end;
+
+		update_lock(opo->lp, X_WAIT);
+	}
+
+	return 0;
+}
+
+/* go through r->locks to find what needs to be done to extend,
+   shrink, shift, split, etc existing locks (this often involves adding new
+   locks in addition to modifying existing locks. */
+
+static int plock_internal(struct dlm_resource *r, unsigned long owner,
+			  int wait, int ex, uint64_t start, uint64_t end)
+{
+	LIST_HEAD(exist);
+	struct posix_lock *po, *safe, *case4_po = NULL;
+	int error = 0;
+
+	list_for_each_entry_safe(po, safe, &r->locks, list) {
+		if (po->owner != owner)
+			continue;
+		if (!ranges_overlap(po->start, po->end, start, end))
+			continue;
+
+		/* existing range (RE) overlaps new range (RN) */
+
+		switch(overlap_type(start, end, po->start, po->end)) {
+
+		case 0:
+			if (po->ex == ex)
+				goto out;
+
+			/* ranges the same - just update the existing lock */
+			po->ex = ex;
+			update_lock(po->lp, wait);
+			goto out;
+
+		case 1:
+			if (po->ex == ex)
+				goto out;
+
+			error = lock_case1(po, r, owner, wait, ex, start, end);
+			goto out;
+
+		case 2:
+			if (po->ex == ex)
+				goto out;
+
+			error = lock_case2(po, r, owner, wait, ex, start, end);
+			goto out;
+
+		case 3:
+			list_move_tail(&po->list, &exist);
+			break;
+
+		case 4:
+			DLM_ASSERT(!case4_po, );
+			case4_po = po;
+			list_move_tail(&po->list, &exist);
+			break;
+
+		default:
+			error = -1;
+			goto out;
+		}
+	}
+
+	if (case4_po)
+		error = lock_case4(case4_po, &exist, r, owner, wait, ex,
+				   start, end);
+	else if (!list_empty(&exist))
+		error = lock_case3(&exist, r, owner, wait, ex, start, end);
+	else
+		add_lock(r, owner, wait, ex, start, end, 0);
+
+ out:
+	return error;
+}
+
+static int punlock_internal(struct dlm_resource *r, unsigned long owner,
+	  		    uint64_t start, uint64_t end)
+{
+	struct posix_lock *po, *safe;
+	int error = 0;
+
+	list_for_each_entry_safe(po, safe, &r->locks, list) {
+		if (po->owner != owner)
+			continue;
+		if (!ranges_overlap(po->start, po->end, start, end))
+			continue;
+
+		/* existing range (RE) overlaps new range (RN) */
+
+		switch(overlap_type(start, end, po->start, po->end)) {
+
+		case 0:
+			/* ranges the same - just remove the existing lock */
+
+			list_del(&po->list);
+			remove_lock(po->lp);
+			goto out;
+
+		case 1:
+			/* RN within RE and starts or ends on RE boundary -
+			 * shrink and update RE */
+
+			shrink_range(po, start, end);
+			update_lock(po->lp, X_WAIT);
+			goto out;
+
+		case 2:
+			/* RN within RE - shrink and update RE to be front
+			 * fragment, and add a new lock for back fragment */
+
+			add_lock(r, owner, po->ex ? WAIT : X_WAIT, po->ex,
+				 end+1, po->end, HEAD);
+
+			po->end = start - 1;
+			update_lock(po->lp, X_WAIT);
+			goto out;
+
+		case 3:
+			/* RE within RN - remove RE, then continue checking
+			 * because RN could cover other locks */
+
+			list_del(&po->list);
+			remove_lock(po->lp);
+			continue;
+
+		case 4:
+			/* front of RE in RN, or end of RE in RN - shrink and
+			 * update RE, then continue because RN could cover
+			 * other locks */
+
+			shrink_range(po, start, end);
+			update_lock(po->lp, X_WAIT);
+			continue;
+
+		default:
+			error = -1;
+			goto out;
+		}
+	}
+
+ out:
+	return error;
+}
+
+static int wait_local(struct dlm_resource *r, unsigned long owner,
+		      uint64_t start, uint64_t end, int ex)
+{
+	DECLARE_WAITQUEUE(wait, current);
+	int error = 0;
+
+	add_wait_queue(&r->waiters, &wait);
+
+	for (;;) {
+		set_current_state(TASK_INTERRUPTIBLE);
+
+		if (!local_conflict(r->dlm, r, &r->name, owner, start, end, ex))
+			break;
+
+		if (signal_pending(current)) {
+			up(&r->sema);
+			error = -EINTR;
+			break;
+		}
+
+		up(&r->sema);
+		schedule();
+		down(&r->sema);
+	}
+
+	remove_wait_queue(&r->waiters, &wait);
+	set_current_state(TASK_RUNNING);
+	return error;
+}
+
+int lm_dlm_plock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		 struct file *file, int cmd, struct file_lock *fl)
+{
+	dlm_t *dlm = (dlm_t *) lockspace;
+	unsigned long owner = (unsigned long) fl->fl_owner;
+	int wait = IS_SETLKW(cmd);
+	int ex = (fl->fl_type == F_WRLCK);
+	uint64_t start = fl->fl_start, end = fl->fl_end;
+	struct dlm_resource *r;
+	int error;
+
+	log_debug("en plock %x,%"PRIx64"", name->ln_type, name->ln_number);
+
+	error = get_resource(dlm, name, CREATE, &r);
+	if (error)
+		goto out;
+
+	error = down_interruptible(&r->sema);
+	if (error)
+		goto out_put;
+
+	/* We wait here until we aren't blocked by any other local locks.
+	   Then we can request the lock from the dlm, request the vfs lock
+	   (without it blocking) and release r->sema before waiting on the dlm
+	   request.  [We can't release the semaphore between the dlm and vfs
+	   requests, but we must release it before waiting for the ast.] */
+
+	error = local_conflict(dlm, r, name, owner, start, end, ex);
+	if (error) {
+		if (!wait) {
+			error = -EAGAIN;
+			goto out_up;
+		}
+		error = wait_local(r, owner, start, end, ex);
+		if (error)
+			goto out_put;
+		/* wait_local returns with r->sema held if no error */
+	}
+
+	error = lock_resource(r);
+	if (error)
+		goto out_up;
+
+	if (!wait && global_conflict(dlm, name, owner, start, end, ex)) {
+		error = -EAGAIN;
+		unlock_resource(r);
+		goto out_up;
+	}
+
+	/* If NO_WAIT all requests should return immediately.
+	   If WAIT all requests go on r->async_locks which we wait on in
+	   wait_async_locks().  This means DLM should not return -EAGAIN and we
+	   should never block waiting for a plock to be released until we call
+	   wait_async_list(). */
+
+	error = plock_internal(r, owner, wait, ex, start, end);
+	unlock_resource(r);
+
+	if (!error) {
+		/* this won't block due to wait_local above and not yet
+		   having released r->sema */
+		if (posix_lock_file_wait(file, fl) < 0)
+			log_error("lm_dlm_plock: vfs lock error %x,%"PRIx64"",
+				  name->ln_type, name->ln_number);
+	}
+
+ out_up:
+	up(&r->sema);
+	wait_async_list(r, owner);
+	wake_up_all(&r->waiters);
+ out_put:
+	put_resource(r);
+ out:
+	log_debug("ex plock %d", error);
+	return error;
+}
+
+int lm_dlm_punlock(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		   struct file *file, struct file_lock *fl)
+{
+	dlm_t *dlm = (dlm_t *) lockspace;
+	unsigned long owner = (unsigned long) fl->fl_owner;
+	uint64_t start = fl->fl_start, end = fl->fl_end;
+	struct dlm_resource *r;
+	int error;
+
+	log_debug("en punlock %x,%"PRIx64"", name->ln_type, name->ln_number);
+
+	error = get_resource(dlm, name, NO_CREATE, &r);
+	if (error)
+		goto out;
+
+	down(&r->sema);
+
+	if (posix_lock_file_wait(file, fl) < 0)
+		log_error("lm_dlm_punlock: vfs unlock error %x,%"PRIx64"",
+			  name->ln_type, name->ln_number);
+
+	error = lock_resource(r);
+	if (error)
+		goto out_up;
+
+	error = punlock_internal(r, owner, start, end);
+	unlock_resource(r);
+
+ out_up:
+	up(&r->sema);
+	wait_async_list(r, owner);
+	wake_up_all(&r->waiters);
+	put_resource(r);
+ out:
+	log_debug("ex punlock %d", error);
+	return error;
+}
+
+static void query_ast(void *astargs)
+{
+	dlm_lock_t *lp = (dlm_lock_t *) astargs;;
+	complete(&lp->uast_wait);
+}
+
+static int get_global_conflict(dlm_t *dlm, struct lm_lockname *name,
+			       unsigned long owner, uint64_t *start,
+			       uint64_t *end, int *ex, unsigned long *rowner)
+{
+	dlm_lock_t *lp;
+	struct dlm_queryinfo qinfo;
+	struct dlm_lockinfo *lki;
+	int query = 0, s, error;
+
+	/* acquire a null lock on which to base the query */
+
+	lp = find_null_lock(dlm, name);
+	if (!lp) {
+		error = create_lp(dlm, name, &lp);
+		if (error)
+			goto ret;
+
+		lp->req = DLM_LOCK_NL;
+		lp->lkf = DLM_LKF_EXPEDITE;
+		set_bit(LFL_INLOCK, &lp->flags);
+		do_dlm_lock_sync(lp, NULL);
+	}
+
+	/* do query, repeating if insufficient space */
+
+	query = DLM_LOCK_THIS | DLM_QUERY_QUEUE_GRANTED |
+		DLM_QUERY_LOCKS_HIGHER;
+
+	for (s = 16; s < dlm->max_nodes + 1; s += 16) {
+
+		lki = kmalloc(s * sizeof(struct dlm_lockinfo), GFP_KERNEL);
+		if (!lki) {
+			error = -ENOMEM;
+			goto out;
+		}
+		memset(lki, 0, s * sizeof(struct dlm_lockinfo));
+		memset(&qinfo, 0, sizeof(qinfo));
+		qinfo.gqi_locksize = s;
+		qinfo.gqi_lockinfo = lki;
+
+		init_completion(&lp->uast_wait);
+		error = dlm_query(dlm->gdlm_lsp, &lp->lksb, query, &qinfo,
+			   	   query_ast, (void *) lp);
+		if (error) {
+			kfree(lki);
+			goto out;
+		}
+		wait_for_completion(&lp->uast_wait);
+		error = lp->lksb.sb_status;
+
+		if (!error)
+			break;
+		kfree(lki);
+		if (error != -E2BIG)
+			goto out;
+	}
+
+	/* check query results for blocking locks */
+
+	error = 0;
+
+	for (s = 0; s < qinfo.gqi_lockcount; s++) {
+
+		lki = &qinfo.gqi_lockinfo[s];
+
+		if (!ranges_overlap(*start, *end, lki->lki_grrange.ra_start,
+				    lki->lki_grrange.ra_end))
+			continue;
+
+		if (lki->lki_node == dlm->our_nodeid)
+			continue;
+
+		if (lki->lki_grmode == DLM_LOCK_EX || *ex) {
+			*start = lki->lki_grrange.ra_start;
+			*end = lki->lki_grrange.ra_end;
+			*ex = (lki->lki_grmode == DLM_LOCK_EX) ? 1 : 0;
+			*rowner = lki->lki_node;
+			error = -EAGAIN;
+			break;
+		}
+	}
+
+	kfree(qinfo.gqi_lockinfo);
+
+	log_debug("global conflict %d %"PRIx64"-%"PRIx64" ex %d own %lu",
+		  error, *start, *end, *ex, *rowner);
+ out:
+	keep_null_lock(dlm, lp);
+ ret:
+	return error;
+}
+
+static int get_local_conflict(dlm_t *dlm, struct dlm_resource *r,
+			      struct lm_lockname *name, unsigned long owner,
+			      uint64_t *start, uint64_t *end, int *ex,
+			      unsigned long *rowner)
+{
+	struct posix_lock *po;
+	int found = FALSE;
+
+	list_for_each_entry(po, &r->locks, list) {
+		if (po->owner == owner)
+			continue;
+		if (!ranges_overlap(po->start, po->end, *start, *end))
+			continue;
+
+		if (*ex || po->ex) {
+			*start = po->start;
+			*end = po->end;
+			*ex = po->ex;
+			*rowner = po->owner;
+			found = TRUE;
+			break;
+		}
+	}
+	return found;
+}
+
+static int do_plock_get(dlm_t *dlm, struct lm_lockname *name,
+			unsigned long owner, uint64_t *start, uint64_t *end,
+			int *ex, unsigned long *rowner)
+{
+	struct dlm_resource *r;
+	int error, found;
+
+	error = get_resource(dlm, name, NO_CREATE, &r);
+	if (!error) {
+		error = down_interruptible(&r->sema);
+		if (error) {
+			put_resource(r);
+			goto out;
+		}
+
+		found = get_local_conflict(dlm, r, name, owner, start, end, ex,
+					   rowner);
+		up(&r->sema);
+		put_resource(r);
+		if (found)
+			goto out;
+	}
+
+	error = get_global_conflict(dlm, name, owner, start, end, ex, rowner);
+	if (error == -EAGAIN) {
+		log_debug("pl get global conflict %"PRIx64"-%"PRIx64" %d %lu",
+			  *start, *end, *ex, *rowner);
+		error = 1;
+	}
+ out:
+	return error;
+}
+
+static int local_conflict(dlm_t *dlm, struct dlm_resource *r,
+			  struct lm_lockname *name, unsigned long owner,
+			  uint64_t start, uint64_t end, int ex)
+{
+	uint64_t get_start = start, get_end = end;
+	unsigned long get_owner = 0;
+	int get_ex = ex;
+
+	return get_local_conflict(dlm, r, name, owner,
+				  &get_start, &get_end, &get_ex, &get_owner);
+}
+
+static int global_conflict(dlm_t *dlm, struct lm_lockname *name,
+			   unsigned long owner, uint64_t start, uint64_t end,
+			   int ex)
+{
+	uint64_t get_start = start, get_end = end;
+	unsigned long get_owner = 0;
+	int get_ex = ex;
+
+	return get_global_conflict(dlm, name, owner,
+				   &get_start, &get_end, &get_ex, &get_owner);
+}
+
+int lm_dlm_plock_get(lm_lockspace_t *lockspace, struct lm_lockname *name,
+		     struct file *file, struct file_lock *fl)
+{
+	dlm_t *dlm = (dlm_t *) lockspace;
+	unsigned long pid;
+	int ex, error;
+
+	ex = (fl->fl_type == F_WRLCK) ? 1 : 0;
+
+	error = do_plock_get(dlm, name, fl->fl_pid, &fl->fl_start,
+			     &fl->fl_end, &ex, &pid);
+	if (error < 0)
+		return error;
+	if (error == 0)
+		fl->fl_type = F_UNLCK;
+	else {
+		fl->fl_type = (ex) ? F_WRLCK : F_RDLCK;
+		fl->fl_pid = pid;
+	}
+
+	return 0;
+}
diff -urN linux-orig/fs/gfs_locking/lock_dlm/thread.c linux-patched/fs/gfs_locking/lock_dlm/thread.c
--- linux-orig/fs/gfs_locking/lock_dlm/thread.c	1969-12-31 18:00:00.000000000 -0600
+++ linux-patched/fs/gfs_locking/lock_dlm/thread.c	2004-12-17 21:09:42.958439548 -0600
@@ -0,0 +1,432 @@
+/******************************************************************************
+*******************************************************************************
+**
+**  Copyright (C) Sistina Software, Inc.  1997-2003  All rights reserved.
+**  Copyright (C) 2004 Red Hat, Inc.  All rights reserved.
+**
+**  This copyrighted material is made available to anyone wishing to use,
+**  modify, copy, or redistribute it subject to the terms and conditions
+**  of the GNU General Public License v.2.
+**
+*******************************************************************************
+******************************************************************************/
+
+#include "lock_dlm.h"
+
+/* 
+ * Run in dlm_async thread 
+ */
+
+/**
+ * queue_submit - add lock request to queue for dlm_async thread
+ * @lp: DLM lock
+ *
+ * A lock placed on this queue is re-submitted to DLM as soon as
+ * dlm_async thread gets to it.  
+ */
+
+static void queue_submit(dlm_lock_t *lp)
+{
+	dlm_t *dlm = lp->dlm;
+
+	spin_lock(&dlm->async_lock);
+	list_add_tail(&lp->slist, &dlm->submit);
+	set_bit(LFL_SLIST, &lp->flags);
+	spin_unlock(&dlm->async_lock);
+	wake_up(&dlm->wait);
+}
+
+/**
+ * process_blocking - processing of blocking callback
+ * @lp: DLM lock
+ *
+ */
+
+static void process_blocking(dlm_lock_t *lp, int bast_mode)
+{
+	dlm_t *dlm = lp->dlm;
+	unsigned int cb;
+
+	switch (make_lmstate(bast_mode)) {
+	case LM_ST_EXCLUSIVE:
+		cb = LM_CB_NEED_E;
+		break;
+	case LM_ST_DEFERRED:
+		cb = LM_CB_NEED_D;
+		break;
+	case LM_ST_SHARED:
+		cb = LM_CB_NEED_S;
+		break;
+	default:
+		DLM_ASSERT(0, printk("unknown bast mode %u\n", lp->bast_mode););
+	}
+
+	dlm->fscb(dlm->fsdata, cb, &lp->lockname);
+}
+
+/**
+ * process_complete - processing of completion callback for a lock request
+ * @lp: DLM lock
+ *
+ */
+
+static void process_complete(dlm_lock_t *lp)
+{
+	dlm_t *dlm = lp->dlm;
+	struct lm_async_cb acb;
+	int16_t prev_mode = lp->cur;
+
+	memset(&acb, 0, sizeof(acb));
+
+	/*
+	 * This is an AST for an unlock.
+	 */
+
+	if (test_and_clear_bit(LFL_DLM_UNLOCK, &lp->flags)) {
+		if (lp->lksb.sb_status == -DLM_ECANCEL) {
+			printk("lock_dlm: -DLM_ECANCEL num=%x,%"PRIx64"\n",
+			       lp->lockname.ln_type, lp->lockname.ln_number);
+		} else {
+			DLM_ASSERT(lp->lksb.sb_status == -DLM_EUNLOCK,
+				   printk("num=%x,%"PRIx64" status=%d\n",
+					  lp->lockname.ln_type,
+					  lp->lockname.ln_number,
+					  lp->lksb.sb_status););
+			lp->cur = DLM_LOCK_IV;
+			lp->req = DLM_LOCK_IV;
+			lp->lksb.sb_lkid = 0;
+			atomic_dec(&dlm->lock_count);
+		}
+
+		if (test_and_clear_bit(LFL_UNLOCK_SYNC, &lp->flags)) {
+			complete(&lp->uast_wait);
+			return;
+		}
+
+		if (test_and_clear_bit(LFL_UNLOCK_DELETE, &lp->flags)) {
+			delete_lp(lp);
+			return;
+		}
+
+		goto out;
+	}
+
+	if (lp->lksb.sb_flags & DLM_SBF_VALNOTVALID)
+		memset(lp->lksb.sb_lvbptr, 0, DLM_LVB_LEN);
+
+	if (lp->lksb.sb_flags & DLM_SBF_ALTMODE) {
+		if (lp->req == DLM_LOCK_PR)
+			lp->req = DLM_LOCK_CW;
+		else if (lp->req == DLM_LOCK_CW)
+			lp->req = DLM_LOCK_PR;
+	}
+
+	/*
+	 * A canceled lock request.  The lock was just taken off the delayed
+	 * list and was never even submitted to dlm.
+	 */
+
+	if (test_and_clear_bit(LFL_CANCEL, &lp->flags)) {
+		lp->req = lp->cur;
+		acb.lc_ret |= LM_OUT_CANCELED;
+		goto out;
+	}
+
+	/*
+	 * An error occured.
+	 */
+
+	if (lp->lksb.sb_status) {
+		lp->req = lp->cur;
+		if (lp->cur == DLM_LOCK_IV)
+			lp->lksb.sb_lkid = 0;
+
+		if ((lp->lksb.sb_status == -EAGAIN) &&
+		    (lp->lkf & DLM_LKF_NOQUEUE)) {
+			/* a "normal" error */
+		} else
+			printk("lock_dlm: process_complete error id=%x "
+			       "status=%d\n", lp->lksb.sb_lkid,
+			       lp->lksb.sb_status);
+		goto out;
+	}
+
+	/*
+	 * This is an AST for an EX->EX conversion for sync_lvb from GFS.
+	 */
+
+	if (test_and_clear_bit(LFL_SYNC_LVB, &lp->flags)) {
+		complete(&lp->uast_wait);
+		return;
+	}
+
+	/*
+	 * A lock has been demoted to NL because it initially completed during
+	 * BLOCK_LOCKS.  Now it must be requested in the originally requested
+	 * mode.
+	 */
+
+	if (test_and_clear_bit(LFL_REREQUEST, &lp->flags)) {
+
+		DLM_ASSERT(lp->req == DLM_LOCK_NL,);
+		DLM_ASSERT(lp->prev_req > DLM_LOCK_NL,);
+
+		lp->cur = DLM_LOCK_NL;
+		lp->req = lp->prev_req;
+		lp->prev_req = DLM_LOCK_IV;
+		lp->lkf &= ~DLM_LKF_CONVDEADLK;
+
+		set_bit(LFL_NOCACHE, &lp->flags);
+
+		if (test_bit(DFL_BLOCK_LOCKS, &dlm->flags) &&
+		    !test_bit(LFL_NOBLOCK, &lp->flags))
+			queue_delayed(lp, QUEUE_LOCKS_BLOCKED);
+		else
+			queue_submit(lp);
+		return;
+	}
+
+	/* 
+	 * A request is granted during dlm recovery.  It may be granted
+	 * because the locks of a failed node were cleared.  In that case,
+	 * there may be inconsistent data beneath this lock and we must wait
+	 * for recovery to complete to use it.  When gfs recovery is done this
+	 * granted lock will be converted to NL and then reacquired in this
+	 * granted state.
+	 */
+
+	if (test_bit(DFL_BLOCK_LOCKS, &dlm->flags) &&
+	    !test_bit(LFL_NOBLOCK, &lp->flags) &&
+	    lp->req != DLM_LOCK_NL) {
+
+		lp->cur = lp->req;
+		lp->prev_req = lp->req;
+		lp->req = DLM_LOCK_NL;
+		lp->lkf |= DLM_LKF_CONVERT;
+		lp->lkf &= ~DLM_LKF_CONVDEADLK;
+
+		log_debug("rereq %x,%"PRIx64" id %x %d,%d\n",
+			  lp->lockname.ln_type, lp->lockname.ln_number,
+			  lp->lksb.sb_lkid, lp->cur, lp->req);
+
+		set_bit(LFL_REREQUEST, &lp->flags);
+		queue_submit(lp);
+		return;
+	}
+
+	/*
+	 * DLM demoted the lock to NL before it was granted so GFS must be
+	 * told it cannot cache data for this lock.
+	 */
+
+	if (lp->lksb.sb_flags & DLM_SBF_DEMOTED)
+		set_bit(LFL_NOCACHE, &lp->flags);
+
+      out:
+
+	/*
+	 * This is an internal lock_dlm lock (for jid's or plock's)
+	 */
+
+	if (test_bit(LFL_INLOCK, &lp->flags)) {
+		clear_bit(LFL_NOBLOCK, &lp->flags);
+		lp->cur = lp->req;
+		complete(&lp->uast_wait);
+		return;
+	}
+
+	/*
+	 * Normal completion of a lock request.  Tell GFS it now has the lock.
+	 */
+
+	clear_bit(LFL_NOBLOCK, &lp->flags);
+	lp->cur = lp->req;
+
+	acb.lc_name = lp->lockname;
+	acb.lc_ret |= make_lmstate(lp->cur);
+
+	if (!test_and_clear_bit(LFL_NOCACHE, &lp->flags) &&
+	    (lp->cur > DLM_LOCK_NL) && (prev_mode > DLM_LOCK_NL))
+		acb.lc_ret |= LM_OUT_CACHEABLE;
+
+	if (prev_mode == DLM_LOCK_IV && lp->cur > DLM_LOCK_IV)
+		atomic_inc(&dlm->lock_count);
+
+	dlm->fscb(dlm->fsdata, LM_CB_ASYNC, &acb);
+}
+
+/**
+ * no_work - determine if there's work for the dlm_async thread
+ * @dlm:
+ *
+ * Returns: 1 if no work, 0 otherwise
+ */
+
+static __inline__ int no_work(dlm_t *dlm)
+{
+	int ret;
+
+	spin_lock(&dlm->async_lock);
+
+	ret = list_empty(&dlm->complete) &&
+	    list_empty(&dlm->blocking) &&
+	    list_empty(&dlm->submit) &&
+	    list_empty(&dlm->starts) && !test_bit(DFL_MG_FINISH, &dlm->flags);
+
+	spin_unlock(&dlm->async_lock);
+
+	return ret;
+}
+
+static __inline__ int check_drop(dlm_t *dlm)
+{
+	if (!dlm->drop_locks_count)
+		return FALSE;
+
+	if (check_timeout(dlm->drop_time, dlm->drop_locks_period)) {
+		dlm->drop_time = jiffies;
+		if (atomic_read(&dlm->lock_count) >= dlm->drop_locks_count)
+			return TRUE;
+	}
+	return FALSE;
+}
+
+static __inline__ int check_shrink(dlm_t *dlm)
+{
+	if (check_timeout(dlm->shrink_time, SHRINK_CACHE_TIME)){
+		dlm->shrink_time = jiffies;
+		return TRUE;
+	}
+	return FALSE;
+}
+
+/**
+ * dlm_async - thread for a variety of asynchronous processing
+ * @data:
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+static int dlm_async(void *data)
+{
+	dlm_t *dlm = (dlm_t *) data;
+	dlm_lock_t *lp = NULL;
+	dlm_start_t *ds = NULL;
+	uint8_t complete, blocking, submit, start, finish, drop, shrink;
+	DECLARE_WAITQUEUE(wait, current);
+
+	while (!kthread_should_stop()) {
+		set_current_state(TASK_INTERRUPTIBLE);
+		add_wait_queue(&dlm->wait, &wait);
+		if (no_work(dlm))
+			schedule();
+		remove_wait_queue(&dlm->wait, &wait);
+		set_current_state(TASK_RUNNING);
+
+		complete = blocking = submit = start = finish = 0;
+		drop = shrink = 0;
+
+		spin_lock(&dlm->async_lock);
+
+		if (!list_empty(&dlm->complete)) {
+			lp = list_entry(dlm->complete.next, dlm_lock_t, clist);
+			list_del(&lp->clist);
+			clear_bit(LFL_CLIST, &lp->flags);
+			complete = 1;
+		} else if (!list_empty(&dlm->blocking)) {
+			lp = list_entry(dlm->blocking.next, dlm_lock_t, blist);
+			list_del(&lp->blist);
+			clear_bit(LFL_BLIST, &lp->flags);
+			blocking = lp->bast_mode;
+			lp->bast_mode = 0;
+		} else if (!list_empty(&dlm->submit)) {
+			lp = list_entry(dlm->submit.next, dlm_lock_t, slist);
+			list_del(&lp->slist);
+			clear_bit(LFL_SLIST, &lp->flags);
+			submit = 1;
+		} else if (!test_bit(DFL_RECOVER, &dlm->flags) &&
+			   !list_empty(&dlm->starts)) {
+			ds = list_entry(dlm->starts.next, dlm_start_t, list);
+			list_del(&ds->list);
+			set_bit(DFL_RECOVER, &dlm->flags);
+			start = 1;
+		} else if (test_and_clear_bit(DFL_MG_FINISH, &dlm->flags)) {
+			finish = 1;
+		}
+
+		/* Don't get busy doing this stuff during recovery. */
+		if (!test_bit(DFL_RECOVER, &dlm->flags)) {
+			drop = check_drop(dlm);
+			shrink = check_shrink(dlm);
+		}
+		spin_unlock(&dlm->async_lock);
+
+		if (complete)
+			process_complete(lp);
+
+		else if (blocking)
+			process_blocking(lp, blocking);
+
+		else if (submit)
+			process_submit(lp);
+
+		else if (start)
+			process_start(dlm, ds);
+
+		else if (finish)
+			process_finish(dlm);
+
+		if (drop)
+			dlm->fscb(dlm->fsdata, LM_CB_DROPLOCKS, NULL);
+		if (shrink)
+			shrink_null_cache(dlm);
+
+		schedule();
+	}
+
+	return 0;
+}
+
+/**
+ * init_async_thread
+ * @dlm:
+ *
+ * Returns: 0 on success, -EXXX on failure
+ */
+
+int init_async_thread(dlm_t *dlm)
+{
+	struct task_struct *p;
+	int error;
+
+	p = kthread_run(dlm_async, dlm, "lock_dlm1");
+	error = IS_ERR(p);
+	if (error) {
+		log_all("can't start lock_dlm1 daemon %d", error);
+		return error;
+	}
+	dlm->thread1 = p;
+
+	p = kthread_run(dlm_async, dlm, "lock_dlm2");
+	error = IS_ERR(p);
+	if (error) {
+		log_all("can't start lock_dlm2 daemon %d", error);
+		kthread_stop(dlm->thread1);
+		return error;
+	}
+	dlm->thread2 = p;
+
+	return 0;
+}
+
+/**
+ * release_async_thread
+ * @dlm:
+ *
+ */
+
+void release_async_thread(dlm_t *dlm)
+{
+	kthread_stop(dlm->thread1);
+	kthread_stop(dlm->thread2);
+}
